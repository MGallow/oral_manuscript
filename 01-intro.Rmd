
# Introduction {#intro}

In the simplest terms, inferential statistics boils down to two primary components: estimation and prediction. Estimation tends to refer to the process by which we infer properties and behavior about individual parameters. For instance, decision theory tell us that the method by which we estimate parameters should be one that incurs low risk or variance. Prediction, on the other hand, tends to refer to the process of determining the likelihood of an unknown or unforseeable event. Right now, prediction is king. The demand for good, predictive models has sparked huge investments in research and given rise to communities - like the machine learning community - whose primary goal is establishing good predictive models.

However, though prediction is getting a lot of attention, these two components are not entirely distinct and often go hand-in-hand. In fact, the success of many predictive models is largely contingent upon the ability to be efficient and proficient in estimation. That is, the succces and reliability of prediction is often a product of good estimation.

For instance, many statistical models require the estimation of the co-variability and interaction between variables. This statistical object often comes in the form of a precision matrix ($\Omega$) - or the inverse of the covariance matrix. For this reason, in the last decade there has been an ever-expanding community devoted to matrix precision estimation. Like the covariance matrix, the precision matrix offers a powerful way of presenting the covariance - or pairwise variability between any two random variables.

Fisher's linear discriminant analysis (LDA) is one of these statistical models. Under the assumption of normal data, the model aims to classify observations into two or more populations of data. 

The central theme of this thesis is on precision matrix estimation but also expanding on a paper written by Aaron Molstad and Adam Rothman in 2017. In this paper, @molstad2017shrinking focused on the fact that only a "characteristic" of a precision matrix is required to fit these models.

- From @molstad2017shrinking: "To fit many predictive models, only a characteristic of the population precision matrix needs to be estimated. For example, in binary linear discriminant analysis, the population precision matrix is needed for prediction only through the product of the precision matrix and the difference between the two conditional distribution mean vectors. Many authors have proposed methods that directly estimate this characteristic @cai2011direct; @fan2012road; @mai2012direct."

2. From @molstad2017shrinking: "@chen2016regularized proposed a method for estimating the characteristic $\Omega_{*}\mu_{*}$ directly, but like the direct linear discriminant methods, this apporach does not lead to an estimate of $\Omega_{*}$."

With the goal of improving performance..

They offer a framework to do other things...

Their approach/methodoloy was published in BLAH. Shrinking Characteristic of precision matrix estimators.


The purpose of this work began with the desire to expand on the work in @molstad2017shrinking and to explore avenues that were mentioned but were not further investigated. One of these research directions was the application of their shrinking characteristics approach to regression. However, in order to get to that point and to build up the knowledge to understand and contribute new material, there were a number of concepts that needed to be understood along the way. This document will follow that journey.

We will begin chapter 2 with a brief introduction to the area of precision matrix estimation and the gaussian log-likelihood function. This section will include mentions of other estimation methods and algorithms but most discussion we will spent on the ADMM algorithm. This discussion will be useful when we begin elaborating on the shrinking characteristics approach (sometimes referred to as SCPME), the augmented ADMM algorithm, and later the application to regression. Lastly, the document will end with two brief tutorials for the R packages `ADMMsigma` and `SCPME`. These packages were developed by myself to aid in simulation experiments and make it easier to branch into related research directions. Both packages have since been published on CRAN.



### Notation and Definitions

For strictly positive integers $n$ and $p$, we will denote $\mathbb{R}^{n \times p}$ as the class of real matrices with dimenson $n \times p$. The class of real, symmetric matrices with dimension $p \times p$ will be denoted as $\mathbb{S}^{p}$ and $\mathbb{S}^{p}_{+}$ if we further require the object to be positive definite. The sample size in a given data set will most often correspond with $n$ and $p$ will correspond with the dimension of the prediction vector. If the dimension of the response vector exceeds one, we will denote it as $r$.

Most matrices will take the form of either $\Sigma$, which should be taken to represent the population covariance matrix, or $\Omega$, which should be taken to represent the population *precision* matrix. Note that the precision matrix is simply the inverse of the covariance matrix ($\Omega \equiv \Sigma^{-1}$). We will sometimes add a subscript star if the object is oracle - or known- a priori ($\Omega_{*}$) and the population object's estimator that minimizes/maximizes a pre-specified objective function will have a hat attached to it ($\hat{\Omega}$).

There will be significant matrix algebra notation throughout the document. The trace operator which sums the diagonal elements of a matrix will take the form $tr\left(\cdot\right)$ and the *exponential* trace operator will be denoted similarly as $etr\left(\cdot\right)$. The determinant of a matrix $\mathbf{A}$ will be denoted as $\left|\mathbf{A}\right|$ but may also take the form as simply $det\left(\mathbf{A}\right)$. The kronecker product of two matrices $\mathbf{A}$ and $\mathbf{B}$ will be denoted as $\mathbf{A} \otimes \mathbf{B}$ and the element-wise product will be denoted as $\mathbf{A} \circ \mathbf{B}$. Lastly, the Frobenius norm which sums the square of all entries in a matrix will be denoted as $\left\|\mathbf{A}\right\|_{F}$ and we will define $\left\|\mathbf{A}\right\|_{1} := \sum_{i, j}\left|\mathbf{A}_{ij}\right|$ where the $i$-$j$th element in matrix $\mathbf{A}$ is denoted as $\left(\mathbf{A}\right)_{ij}$ or simply $\mathbf{A}_{ij}$.
