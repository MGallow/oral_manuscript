
# Introduction {#intro}

In many statistical applications, estimating the covariance matrix, $\Sigma$, for a set of random variables is a critical task. The covariance matrix is useful because it characterizes the *relationship* between variables. For instance, suppose we have three variables $X, Y, \mbox{ and } Z$ and their covariance matrix is of the form

\begin{equation}
\Sigma_{xyz} = \begin{pmatrix}
1 & 0 & 0.5 \\ 
0 & 1 & 0 \\ 
0.5 & 0 & 1
\end{pmatrix}
(\#eq:ex)\notag
\end{equation}

We can gather valuable information from this matrix. First of all, we know that each of the variables has an equal variance of 1. Second,  we know that variables $X$ and $Y$ are likely independent because the covariance between the two is equal to 0. This implies that any information in $X$ is useless in trying to gather information about $Y$. Lastly, we know that variables $X$ and $Z$ are moderately, positively correlated because their covariance is 0.5.

Unfortunately, estimating $\Sigma$ well is often computationally expensive and, in a few settings, extremely challenging. For this reason, emphasis in the literature and elsewhere has been placed on estimating the inverse of $\Sigma$ which we like to denote as $\Omega \equiv \Sigma^{-1}$.

If we have data that is normally distributed with mean $\mu$ and variance $\Sigma$ (that is, $X_{i} \sim N_{p}\left(\mu, \Sigma \right)$), the optimal estimator for $\Omega$ with respect to the log-likelihood is of the form

\begin{equation}
\hat{\Omega}^{MLE} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ tr\left(S\Omega\right) - \log\left|\Omega\right| \right\}
(\#eq:mleoptim)\notag
\end{equation}

where $S$ denotes the usual sample estimator ($S = \sum_{i = 1}^{n}\left(X_{i} - \bar{X} \right)\left(X_{i} - \bar{X} \right)^{T})$). As in regression settings, we can construct *penalized* log-likelihood estimators by adding a penalty term, $P\left(\Omega\right)$, to the log-likelihood so that

\begin{equation}
\hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ tr\left(S\Omega\right) - \log\left|\Omega \right| + P\left( \Omega \right) \right\}
(\#eq:penoptim)\notag
\end{equation}

$P\left( \Omega \right)$ is often of the form $P\left(\Omega \right) = \lambda\|\Omega \|_{F}^{2}/2$ or $P\left(\Omega \right) = \|\Omega\|_{1}$ where $\lambda > 0$, $\left\|\cdot \right\|_{F}^{2}$ is the Frobenius norm and we define $\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|$. These penalties are the ridge and lasso, respectively. The penalty proposed in @molstad2017shrinking, however, is of the form $P\left(\Omega\right) = \left\|A\Omega B - C\right\|_{1}$ so that the general optimization problem is

\begin{equation}
\hat{\Omega} = \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{ tr(S\Omega) - \log\left| \Omega \right| + \lambda\left\| A\Omega B - C \right\|_{1} \right\}
(\#eq:molstadeq)\notag
\end{equation}

`SCPME` is an implementation of the proposed augmented ADMM algorithm in @molstad2017shrinking for solving the previous optimization problem. In addition, this package places a big emphasis on flexibility that allows for rapid experimentation for the end user.

We will illustrate this with a short simulation and show some of the new and interesting estimators for $\Omega$ that are a result of this penalty.

- Why do we care?

- What is SCPME?

The reference we need here is @molstad2017shrinking!

1. From @molstad2017shrinking: "To fit many predictive models, only a characteristic of the population precision matrix needs to be estimated. For example, in binary linear discriminant analysis, the population precision matrix is needed for prediction only through the product of the precision matrix and the difference between the two conditional distribution mean vectors. Many authors have proposed methods that directly estimate this characteristic @cai2011direct; @fan2012road; @mai2012direct."

2. From @molstad2017shrinking: "@chen2016regularized proposed a method for estimating the characteristic $\Omega_{*}\mu_{*}$ directly, but like the direct linear discriminant methods, this apporach does not lead to an estimate of $\Omega_{*}$."

3. Discuss linear regression application.... where $\beta = \Omega_{*}\Sigma_{*XY}$. Like @witten2009covariance, this approach estimates the regression coefficients "using shrinkage estimators of the marginal precision matrix for the predictors".

- Guide

- Model and notation


## Outline


- Why do we care about covariance matrices? Covariance matrices are an object that summarizes the relationship between variables.

- What is a precision matrix? Precision matrix is the inverse of a covariance matrix.

- Precision matrices are powerful, essential in representing relationships in variables

- In fact, required to be estimated to fit many statistical models.

- That is, we must provide an object that accurately summarizes the relationship and interaction between variables in our model.

- Many proposed shrinkage estimators when p is particularly large. Shrinkage.

- However, to fit these models, Molstad and Rothman focused on the fact that onnly a "characteristic" of a precision matrix is required to fit these models. "We propose a framework to shrink a user-specified characteristic of a precision matrix estimator."

- For instance, LDA, linear regression, ...

- Their approach/methodology was published in BLAH. Shrinking characteristic of precision matrix estimators.

- BREAK

- We wanted to explore the regression application that was mentioned but not pursued/investigated

- This document will outline the research and work leading up to that

- The document is outlined as follows...

- We will begin chapter 2 with a brief introduction to precision matrix estimation. This will include mentions of estimation methods/approaches and we will spend a particular amount of time elaborating on one approach which is estimation via the ADMM algorithm. This dicussion will be useful when we circle back to SCPME, the regression application, and the augmented ADMM algorithm developed in BLAH.

- Lastly, the document will end with two brief tutorials for the R packages ADMMsigma and SCPME. These packages, developed by myself, will be mentioned throughout the document and were instrumental in this pursued line of research and facilitated many simulations and research direction explored throughout. Both packages have since been published on CRAN


## Model and Notation
