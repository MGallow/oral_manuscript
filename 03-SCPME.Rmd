
# SCPME

In their 2017 paper, titled *Shrinking Characteristics of Precision Matrix Estimators*, Aaron Molstad, Ph.D. and Professor Adam Rothman outline a framework to estimate *characteristics* of precision matrices. This concept, inspired by others like @cai2011direct, @fan2012road, and @mai2012direct, exploits the fact that in many predictive models estimation of the precision matrix is only necessary through its product with another feature, such as a mean vector. The example they offer in @molstad2017shrinking is in the context of Fisher's linear discriminant analysis model. If a response vector $Y$ is categorical such that $Y$ can take values in $\left\{1, ..., J\right\}$, then the linear discrimnant analysis model assumes that the design matrix $X$ conditional on the response vector $Y$ is normally distributed:

\begin{equation}
X | Y = j \sim N_{p}\left(\mu_{j}, \Omega^{-1}\right)
(\#eq:lda)
\end{equation}

for each $j = 1, ..., J$. One can see from this formulation that clearly an estimation of both $\mu$ and $\Omega$ are required for this model. However, they note that if prediction is the primary concern, then for a given observation $X_{i}$ only the *characteristic* $\Omega\left(\mu_{l} - \mu_{m}\right)$ is needed to discern between response categories $l$ and $m$. In other words, prediction only requires the characteristic $\Omega\left(\mu_{l} - \mu_{m}\right)$ for each $l, m \in \left\{1, ..., J\right\}$ and does *not* require estimation of the full precision matrix $\Omega$. @cai2011direct were among the first authors to propose estimating this characteristic directly but the interesting facet that distinguishes Molstad and Rothman's approach is that their framework simultaneously fits the model in \@ref(eq:lda) and performs variable selection.

The general framework has applications outside of linear discriminant analysis and we will be exploring a regression application in later sections, but first we will outline their approach. The penalty proposed by @molstad2017shrinking is of the form

\begin{equation}
P\left(\Omega\right) = \lambda\left\| A\Omega B - C \right\|_{1}
(\#eq:pen2)
\end{equation}

where $A \in \mathbb{R}^{m \times p}, B \in \mathbb{R}^{p \times q}, \mbox{ and } C \in \mathbb{R}^{m \times q}$ are matrices that are assumed to be known and specified so that solving the full penalized gaussian negative log-likelihood for $\Omega$ results in solving

\begin{equation}
\hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ tr\left(S\Omega\right) - \log\left|\Omega \right| + \lambda\left\| A\Omega B - C \right\|_{1} \right\}
(\#eq:omegaloglik2)
\end{equation}

A similar estimator was proposed by @dalal2017sparse when $C = 0$ but here we do not require it. This form of penalty is particularly useful because it is extremely general. Note that by letting matrices $A = I_{p}, B = I_{p}, \mbox{ and } C = 0$, this penalty reduces to a lasso penalty - but clearly this form allows for much more creative penalties and $A, B, \mbox{ and } C$ can be constructed so that we penalize the sum, absolute value of *many* characteristics of the precision matrix $\Omega$. We will explore how to solve for $\hat{\Omega}$ in \@ref(eq:omegaloglik2) in the next section.



## Augmented ADMM Algorithm

Solving for $\hat{\Omega}$ in \@ref(eq:omegaloglik2) uses what we are going to call an *augmented ADMM algorithm*. Molstad and Rothman do not offer a name for this specific algorithm but it leverages the majorize-minimize principle in one of the steps in the algorithm - augmenting the original ADMM algorithm discussed in the previous chapter. Within the context of the proposed penalty, the original ADMM algorithm for precision matrix estimation would consist of iterating over the following three steps:

\begin{align}
  \Omega^{k + 1} &= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}L_{\rho}(\Omega, Z^{k}, \Lambda^{k}) \\
  Z^{k + 1} &= \arg\min_{Z \in \mathbb{R}^{n \times r}}L_{\rho}(\Omega^{k + 1}, Z, \Lambda^{k}) \\
  \Lambda^{k + 1} &= \Lambda^{k} + \rho\left(A\Omega^{k + 1}B - Z^{k + 1} - C \right)
\end{align}

where $L$, the augmented lagrangian, is defined as

\begin{equation}
L_{\rho}(\Omega, Z, \Lambda) = f\left(\Omega\right) + g\left(Z\right) + tr\left[\Lambda '\left(A\Omega B - Z - C\right)\right] + \frac{\rho}{2}\left\|A\Omega B - Z - C\right\|_{F}^{2}
(\#eq:auglagrange3)
\end{equation}

Similar to the previous chapter, $f\left(\Omega\right) = tr\left(S\Omega\right) - \log\left|\Omega\right|$ and $g\left(Z\right) = \lambda\left\|Z\right\|_{1}$. In fact, the details of the algorithm thus far are identical to the previous approach except that we are replacing $\Omega - Z$ with $A\Omega B - Z - C$ in the augmented lagrangian and the dual update $\Lambda^{k + 1}$.

Instead of solving the first step directly, the authors propose an alternative, approximating objective function, which we will denote as $\tilde{L}$, that is based on the majorize-minimize principle^[Further explanation of the majorizing function \@ref(eq:approx) in section \@ref(taylorsexp)]. The purpose of this approximating function is the desire to solve the first step of the algorithm in closed-form. The ADMM algorithm with this modification based on majorize-minimize principle is also found in @lange2016mm but here we define the approximating function as

\begin{equation}
\begin{split}
  \tilde{L}_{\rho}\left(\Omega, Z^{k}, \Lambda^{k}\right) = f\left(\Omega\right) &+ g\left(Z^{k}\right) + tr\left[(\Lambda^{k})'(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega B - Z^{k} - C \right\|_{F}^{2} \\
  &+ \frac{\rho}{2}vec\left(\Omega - \Omega^{k}\right)' Q\left(\Omega - \Omega^{k}\right)
(\#eq:approx)
\end{split}
\end{equation}

where $Q = \tau I_{p} - \left(A'A \otimes BB'\right)$ and $\tau$ is chosen such that $Q$ is positive definite. Note that if $Q$ is positive definite, then $L_{\rho}\left(\cdot\right) \leq \tilde{L}\left(\cdot\right)$ for all $\Omega$ and $\tilde{L}$ is a majorizing function^[If $Q$ is positive definite, then $vec\left(\Omega - \Omega^{k} \right)'\rho Q\left(\Omega - \Omega^{k} \right)/2 > 0$ since $\rho > 0$ and $vec\left(\Omega - \Omega^{k}\right)$ is always nonzero whenever $\Omega \neq \Omega^{k}$.]. The *augmented ADMM* algorithm developed by Molstad and Rothman, which now includes the majorize-minimize principle, consists of the following repeated iterations:

\begin{equation}
\begin{split}
  \Omega^{k + 1} &= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{tr\left[\left(S + G^{k}\right)\Omega\right] - \log\left|\Omega\right| + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \right\}\\
  Z^{k + 1} &= \arg\min_{Z \in \mathbb{R}^{n \times r}}\left\{\lambda\left\|Z\right\|_{1} + tr\left[(\Lambda^{k})'(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega B - Z^{k} - C \right\|_{F}^{2} \right\} \\
  \Lambda^{k + 1} &= \Lambda^{k} + \rho\left(A\Omega^{k + 1}B - Z^{k + 1} - C \right)
(\#eq:augADMM)\notag
\end{split}
\end{equation}

where $G^{k} = \rho A'\left( A\Omega^{k} B - Z^{k} - C + \rho^{-1}\Lambda^{k} \right)B'$. Each step in this algorithm can now conveniently be solved in closed-form and the full details of each can be found in the appendix \@ref(proofOmegak). The following theorem provides the simplified steps in the algorithm.


```{theorem, name = "Augmented ADMM Algorithm for Shrinking Characteristics of Precision Matrix Estimators."}

Define the soft-thresholding function as $\mbox{soft}(a, b) = \mbox{sign}(a)(\left| a \right| - b)_{+}$ and $S$ as the sample covariance matrix. Set $k = 0$ and initialize $Z^{0}, \Lambda^{0}, \Omega^{0}$, and $\rho$ and repeat steps 1-5 until convergence.

1. Compute $G^{k}$.

\begin{equation}
G^{k} = \rho A'\left( A\Omega^{k} B - Z^{k} - C + \rho^{-1}\Lambda^{k} \right)B'
(\#eq:Gk)\notag
\end{equation}

2. Via spectral decomposition, decompose

\begin{equation}
S + \left( G^{k} + (G^{k})' \right)/2 - \rho\tau\Omega^{k} = VQV'
(\#eq:VQV)\notag
\end{equation}

3. Update $\Omega^{k + 1}$.^[Proof of \@ref(eq:Omegak) in section \@ref(proofOmegak)]

\begin{equation}
\Omega^{k + 1} = V\left( -Q + (Q^{2} + 4\rho\tau I_{p})^{1/2} \right)V' /(2\rho\tau)
(\#eq:Omegak)
\end{equation}

4. Update $Z^{k + 1}$ with element-wise soft-thresholding for the resulting matrix.^[Proof of \@ref(eq:Zk) in section \@ref(proofZk)]

\begin{equation}
Z^{k + 1} = \mbox{soft}\left( A\Omega^{k + 1}B - C + \rho^{-1}\Lambda^{k}, \rho^{-1}\lambda \right)
(\#eq:Zk)
\end{equation}

5. Update $\Lambda^{k + 1}$.

\begin{equation}
\Lambda^{k + 1} = \rho\left( A\Omega^{k + 1} B - Z^{k + 1} - C \right)
(\#eq:Yk)\notag
\end{equation}

```


### Stopping Criterion

A possibe stopping criterion for this framework is one derived from similar optimality conditions used in the previous chapter in section \@ref(ADMMstop). The primal optimality condition here is that $A\Omega^{k + 1}B - Z^{k + 1} - C = 0$ and the two dual optimality conditions are $0 \in \partial f\left(\Omega^{k + 1}\right) + \left(B(\Lambda^{k + 1})'A + A'\Lambda^{k + 1}B' \right)/2$ and $0 \in \partial g\left(Z^{k + 1}\right) - \Lambda^{k + 1}$. Similarly, we will define the left-hand side of the primal optimality condition as the primal residual $r^{k + 1} = A\Omega^{k + 1}B - Z^{k + 1} - C$ and the dual residual^[Proof of \@ref(eq:stopproof) in section \@ref(proofstopproof).] as

\begin{equation}
s^{k + 1} = \frac{\rho}{2}\left( B(Z^{k + 1} - Z^{k})'A + A'(Z^{k + 1} - Z^{k})B' \right)
(\#eq:stopproof)\notag
\end{equation}

For proper convergence, we will require that both residuals are approximately equal to zero. Similar to the stopping criterion discussed previously, one possibility is to set $\epsilon^{rel} = \epsilon^{abs} = 10^{-3}$ and stop the algorithm when $\epsilon^{pri} \leq \left\| r^{k + 1} \right\|_{F}$ and $\epsilon^{dual} \leq \left\| s^{k + 1} \right\|_{F}$ where

\begin{equation}
\begin{split}
  \epsilon^{pri} &= \sqrt{nr}\epsilon^{abs} + \epsilon^{rel}\max\left\{ \left\| A\Omega^{k + 1}B \right\|_{F}, \left\| Z^{k + 1} \right\|_{F}, \left\| C \right\|_{F} \right\} \\
  \epsilon^{dual} &= p\epsilon^{abs} + \epsilon^{rel}\left\| \left( B(\Lambda^{k + 1})'A + A'\Lambda^{k + 1}B' \right)/2 \right\|_{F}
(\#eq:scpmestopping)\notag
\end{split}
\end{equation}



## Regression Illustration

One of the research directions mentioned in @molstad2017shrinking that was not further explored was the application of the SCPME framework to regression. Utilizing the fact that the population regression coefficient matrix $\beta \equiv \Omega_{x}\Sigma_{xy}$ for predictors, $X$, and the responses, $Y$, they point out that their framework could allow for the simultaneous estimation of $\beta$ and $\Omega_{x}$. Like @witten2009covariance, this approach would estimate the forward regression coefficient matrix while using shrinkage estimators for the marginal population precision matrix for the predictors. For example, recall that the general optimization problem outlined in the SCPME framework is to estimate $\hat{\Omega}$ such that

\begin{equation}
  \hat{\Omega} = \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{ tr(S\Omega) - \log\left| \Omega \right| + \lambda\left\| A\Omega B - C \right\|_{1} \right\}
(\#eq:penloglik3)\notag
\end{equation}

If the user specifies that $A = I_{p}, B = \Sigma_{xy}, C = 0, \mbox{ and } \Omega_{x}$ is the precision matrix for the predictors, then the optimization problem of interest is now

\begin{equation}
  \hat{\Omega}_{x} = \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr(S_{x}\Omega_{x}) - \log\left| \Omega_{x} \right| + \lambda\left\| \Omega_{x} \Sigma_{xy} \right\|_{1} \right\} \\
(\#eq:penloglik4)\notag
\end{equation}

Specifically, this optimization problem has the effect of deriving an estimate of $\Omega_{x}$ while assuming sparsity in the forward regression coefficient $\beta$. Of course, in practice we do not know the true covariance matrix $\Sigma_{xy}$ but we might consider using the sample estimate $\hat{\Sigma}_{xy} = \sum_{i = 1}^{n}\left(X_{i} - \bar{X}\right)\left(Y_{i} - \bar{Y}\right)'/n$ in place of $\Sigma_{xy}$. We could then use our estimator, $\hat{\Omega}_{x}$, to construct the estimated forward regression coefficient matrix $\hat{\beta} = \hat{\Omega}_{x}\hat{\Sigma}_{xy}$. Estimators such as these are truly novel an can conveniently be estimated by the SCPME framework. Another such estimator that is a product of this new framework is one where we construct $A \mbox{ and } C$ similarly but take $B = \left[ \Sigma_{xy}, I_{p} \right]$ so that the identity matrix is appended to the cross-covariance matrix of $X$ and $Y$. In this case, not only are we assuming that $\beta$ is sparse, but we are also assuming sparsity in $\Omega$.

\begin{equation}
P_{\lambda}\left(\Omega \right) = \lambda\left\| A\Omega B - C \right\|_{1} = \lambda\left\| \Omega\left[\Sigma_{xy}, I_{p}\right] \right\|_{1} = \lambda\left\| \beta \right\|_{1} + \lambda\left\| \Omega \right\|_{1}
(\#eq:pen4)\notag
\end{equation}

Like before, we could use $\hat{\Sigma}_{xy}$ as a replacement and take our resulting estimator, $\hat{\Omega}_{x}$, to construct the estimated forward regression coefficient matrix $\hat{\beta} = \hat{\Omega}_{x}\hat{\Sigma}_{xy}$. The embedded assumptions here are that not all predictors in $X$ are useful in predicting the response, $Y$, *and* that a number of the predictors are conditionally independent of one another. These are assumptions that are quite reasonable in practice and in the next section we offer a short simulation comparing these new estimators to related and competing prediction methods.



## Simulations

In this simulation, we compare, under various data realizations, the performance of several competing regression prediction methods including the new so-called SCPME regression estimators discussed in the previous section. In total, we consider the following:

 - `OLS` = ordinary least squares estimatior. In high dimensional settings ($p >> n$), the Moore-Penrose estimator is used as a replacement.

 - `ridge` = ridge regression estimator.
 
 - `lasso` = lasso regression estimator.
 
 - `oracleB` = oracle estimator for $\beta$.
 
 - `oracleO` = regression estimator with oracle $\Omega_{x}^{*}$ (let $\beta = \Omega_{x}^{*}\hat{\Sigma}_{xy}$).
 
 - `oracleS` = regression estimator with oracle $\Sigma_{xy}^{*}$ (let $\beta = \hat{\Omega}_{x}\Sigma_{xy}^{*}$) and $\Omega_{x}$ is estimated using an elastic-net penalty.
 
 - `shrinkB` = SCPME regression estimator with penalty $\lambda\left\| \Omega_{x}\hat{\Sigma}_{xy} \right\|_{1}$ so that $\beta = \hat{\Omega}_{x}\hat{\Sigma}_{xy}$.
 
 - `shrinkBS` = SCPME regression estimator with oracle $\Sigma_{xy}^{*}$ so that the penalty is $\lambda\left\| \Omega_{x}\Sigma_{xy}^{*} \right\|_{1}$ and we let $\beta = \hat{\Omega}_{x}\Sigma_{xy}^{*}$.
 
 - `shrinkBO` = SCPME regression estimator with penalty $\lambda\left\| \Omega_{x}[\hat{\Sigma}_{xy}, I_{p}] \right\|_{1}$ so that $\beta = \hat{\Omega}_{x}\hat{\Sigma}_{xy}$.
 
 - `glasso` = graphical lasso estimator with penalty $\lambda\left\| \Omega_{x} \right\|_{1}$ so that $\beta = \hat{\Omega}_{x}\hat{\Sigma}_{xy}$.
 
For each estimator, if selection of a tuning parameter is required, the tuning parameter was chosen so that the mean squared prediction error (MSPE) was minimized over 3-fold cross validation.

The data generating procedure for the simulations is the following. The oracle regression coefficient matrix $\beta^{*}$ was constructed so that $\beta^{*} = \mathbb{B} \circ \mathbb{V}$ where $vec\left( \mathbb{B} \right) \sim N_{pr}\left( 0, I_{p} \otimes I_{r}/\sqrt{p} \right)$ and $\mathbb{V} \in \mathbb{R}^{p \times r}$ is a matrix containing $p$ times $r$ random bernoulli draws with 50\% probability being equal to one. The covariance matrices $\Sigma_{y | x}$ and $\Sigma_{x}$ were constructed so that $\left( \Sigma_{y | x} \right)_{ij} = 0.7^{\left| i - j \right|}$ and $\left( \Sigma_{x} \right)_{ij} = 0.7^{\left| i - j \right|}$, respectively. This ensures that their corresponding precision matrices will be tridiagonal and sparse. Then for 100 independent, identically distributed samples, we had $X_{i} \sim N_{p}\left( 0, \Sigma_{x} \right)$ and $E_{i} \sim N_{r}\left( 0, \Sigma_{y | x} \right)$ so that $\mathbb{Y} = \mathbb{X}\beta + \mathbb{E}$ where $\mathbb{X} \in \mathbb{R}^{n \times p}$ and $\mathbb{Y} \in \mathbb{R}^{n \times r}$ are the matrices with stacked rows $X_{i}$ and $E_{i}$, respectively, for $i = 1, ..., n$ and the script notation denotes the fact that the columns have been centered so as to remove the intercept in our prediction models. A sample of an additional 1000 observations was generated similarly for the testing set. Each prediction method was evaluated on both model error and mean squared prediction error.

Figure \@ref(fig:scpmesim2) displays the model error for each method by dimension of the design matrix. Here we took the sample size equal to $n = 100$ and the response matrix dimension $r = 10$ with each data generating procedure replicated a total of 20 times. Note `OLS` and `oracleS` are not shown due to extremely poor performance. We find that in high dimensional settings, `shrinkBO` and `shrinkB` perform increasingly well relative to the others as the predictor dimension increases. `shrinkBO` performed the best. Interestingly, when $n > p$ `shrinkBO` is still one of the best-performing estimators - though worse than both `lasso` and `ridge` - but the performance of `shrinkB` decreases drastically (plot not shown).

In the high dimension setting, the oracle estimators `oracleO` and `oracleS` performed worse or comparable to the `OLS` estimator. The poor performance of `oracleS` is likely due to the fact that the sample estimate of $\Omega_{x}$ is not identifiable when $p > n$.

The following table shows the average model error for each estimator when $n = 100$, $p = 150$, and $r = 10$.


<!-- <br>\vspace{0.5cm} -->
<!-- ```{r scpmesim1, eval = T, echo = F, fig.cap = "ME by P and R (N = 100, spase = 0.5, sigma = tridiag. High dimension."} -->

<!-- # load data -->
<!-- load("images/sim6.Rdata") -->

<!-- # set P to numeric  -->
<!-- sim6$P %<>% as.character %>% as.numeric -->

<!-- # ME by P and R (N = 100, sparse = 0.5, sigma = "tridiag") -->
<!-- sim6 %>% filter(N == 100, sparse == 0.5, sigma == "tridiag", !model %in% c("OLS", "oracleS"), metric == "ME") %>% group_by(N, P, R, sparse, sigma, model) %>% summarise(mean = mean(Error)) %>% ggplot(aes(P, mean, color = model, linetype = model)) + geom_line() + scale_colour_discrete("") + scale_linetype_manual("", values = rep(c(5,6), 5)) + theme_minimal() + theme(legend.position = "bottom") + facet_wrap(~ R, scales = "free") + xlab("P") + labs(color = "Model") + scale_x_continuous(breaks = c(10, 50, 100, 150)) + ylab("ME") + ggtitle("ME by P and R (N = 100, sparse = 0.5, sigma = tridiag)") -->

<!-- ``` -->
<!-- <br>\vspace{0.5cm} -->

\vspace{0.5cm}
```{r scpmesim2, eval = T, echo = F, fig.cap = "The oracle precision matrices were tri-diagonal with variable dimension (p) and the data was generated with sample size n = 100 and response vector dimension r = 10. The model errors (ME) for each estimator with variable dimension of the design matrix are plotted. shrinkBO and shrinkB were the two best-performing estimators closely follow by the ridge and lasso regression estimators."}

# load  data
load("images/sim6.Rdata")

# set P to numeric
sim6$P %<>% as.character %>% as.numeric

# ME by P (N = 100, R = 10, sparse = 0.5, sigma = "tridiag")
sim6 %>% filter(N == 100, R == 10, sparse == 0.5, sigma == "tridiag", !model %in% c("OLS", "oracleS")) %>% group_by(N, P, R, sparse, sigma, model) %>% summarise(mean = mean(Error)) %>% ggplot(aes(P, mean, color = model, linetype = model)) + geom_line() + scale_colour_discrete("") + scale_linetype_manual("", values = rep(c(5,6), 5)) + theme_minimal() + theme(legend.position = "bottom") + xlab("P") + labs(color = "Model") + scale_x_continuous(breaks = c(10, 50, 100, 150)) + ylab("ME") + ggtitle("Model Error by Dimension")

```

\newpage
```{r, eval = T, echo = F, fig.cap = "ME by P (N = 100, R = 10, sparse = 0.5, sigma = tridiag)"}

# ME table (N = 100, sparse = 0.5, sigma = "tridiag", P = 150, R = 10)
sim6 %>% filter(N == 100, sparse == 0.5, sigma == "tridiag", P == 150, R == 10) %>% group_by(model) %>% summarise(error = mean(Error), sd = sd(Error)) %>% arrange(error) %>% pander(caption = "ME by P (N = 100, R = 10, sparse = 0.5, sigma = tridiag)")

```
\vspace{0.5cm}

<!-- <br>\vspace{0.5cm} -->
<!-- ```{r scpmesim4, eval = T, echo = F, fig.cap = "ME by P and R (N = 1000, spase = 0.5, sigma = tridiag. Low dimension."} -->

<!-- # ME by P and R (N = 1000, sparse = 0.5, sigma = "tridiag") -->
<!-- sim6 %>% filter(N == 1000, sparse == 0.5, model != "OLS") %>% group_by(N, P, R, sparse, sigma, model) %>% summarise(mean = mean(Error)) %>% ggplot(aes(P, mean, color = model, linetype = model)) + geom_line() + scale_colour_discrete("") + scale_linetype_manual("", values = rep(c(5,6), 5)) + theme_minimal() + theme(legend.position = "bottom") + facet_wrap(~ R, scales = "free") + xlab("P") + labs(color = "Model") + scale_x_continuous(breaks = c(10, 50, 100, 150)) + ylab("ME") + ggtitle("ME by P and R (N = 1000, sparse = 0.5, sigma = tridiag)") -->

<!-- ``` -->
<!-- <br>\vspace{0.5cm} -->

<!-- \vspace{0.5cm} -->
<!-- ```{r scpmesim5, eval = T, echo = F, fig.cap = "ME by P (N = 1000, R = 5, spase = 0.5, sigma = tridiag. Low dimension."} -->

<!-- # ME by P (N = 1000, R = 5, sparse = 0.5, sigma = "tridiag") -->
<!-- sim6 %>% filter(N == 1000, R == 5, sparse == 0.5, model != "OLS") %>% group_by(N, P, R, sparse, sigma, model) %>% summarise(mean = mean(Error)) %>% ggplot(aes(P, mean, color = model, linetype = model)) + geom_line() + scale_colour_discrete("") + scale_linetype_manual("", values = rep(c(5,6), 5)) + theme_minimal() + theme(legend.position = "bottom") + xlab("P") + labs(color = "Model") + scale_x_continuous(breaks = c(10, 50, 100, 150)) + ylab("ME") + ggtitle("ME by P (N = 1000, R = 5, sparse = 0.5, sigma = tridiag)") -->

<!-- ``` -->
<!-- \vspace{0.5cm} -->


<!-- \vspace{0.5cm} -->
<!-- ```{r scpmesim6, eval = T, echo = F, fig.cap = "ME by P (N = 1000, R = 5, sparse = 0.5, sigma = tridiag)."} -->

<!-- # ME table (N = 1000, sparse = 0.5, sigma = "tridiag", P = 150, R = 5) -->
<!-- sim6 %>% filter(N == 1000, sparse == 0.5, sigma == "tridiag", P == 150, R == 5) %>% group_by(model) %>% summarise(mean = mean(Error), sd = sd(Error)) %>% arrange(mean) %>% pander(caption = "ME by P (N = 1000, R = 5, sparse = 0.5, sigma = tridiag).") -->

<!-- ``` -->
<!-- \vspace{0.5cm} -->


### Regression Simulations with Covariance Shrinkage

This final simulation was explored due to the fact that each of the SCPME regression estimators require an estimation of the cross-covariance matrix. In the previous simulations, we naively used the maximum likelihood estimate. However, because the data-generating procedure constructs settings that are inherently sparse, we were interested to determine if additional shrinkage of the maximum likelihood estimate for the *cross-covariance* matrix would also prove beneficial. For instance, in a number of settings, we were finding that the estimator `oracleO` was performing worse than `oracleS`. This perhaps suggests that estimating the covariance matrix $\Sigma_{xy}$ well is *more* important than estimating $\Omega$ well. This simulation explores that theory a bit further.

The data-generating procedure for this simulation is similar to previous one but here we take $n = 100$, $p = 200$, $r = 10$, and, in addition, we multiply the population cross-covariance estimator by a factor of $k$ where $k \in (0.1, 0.2, ..., 0.9, 1)$. Figure \@ref(fig:scpmesim7) plots the MSPE for `shrinkB` for each tuning parameter, $\lambda$, and constant, $k$, pair. Figure \@ref(fig:scpmesim9) plots the MSPE similarly for `shrinkBO`.

Interestingly, we find that in this high dimension setting, it does appear that shrinking the sample covariance matrix by a constant factor helps the overall prediction performance of the SCPME estimators. This is indicated by the fact that the optimal constant, $k$, hovers between 0.3 and 0.6 for each of the estimators. We also performed a simulation in low dimensions (not shown) but we did not see the same benefit.

\vspace{0.5cm}
```{r, eval = T, echo = F}

# load  data
load("images/sigma.Rdata")

# design color palette
bluetowhite <- c("#000E29", "white")

# change to numeric
sigma$lam %<>% as.character %>% as.numeric
sigma$const %<>% as.character %>% as.numeric

```

\vspace{0.5cm}
```{r scpmesim7, eval = T, echo = F, fig.cap = "The oracle precision matrices were tri-diagonal with dimension p = 200 and the data was generated with a sample size n = 100 and response vector dimension r = 10. The cross validation MSPE are plotted for each lambda and constant tuning parameter pair. The optimal tuning parameter pair was found to be log10(lam) = -1 and const = 0.6. Note that brighter areas signify smaller losses."}

# shrinkB MSPE by lam and const (N = 100, P = 200, R = 10, sigma = "tridiag")
d = sigma %>% filter(N == 100, P == 200, R == 10, sparse == 0.5, sigma == "tridiag", model == "shrinkB") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean)))

min = d[d$mean == min(d$mean),]

d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkB Covariance Shrinkage", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = ""))

```
\vspace{0.5cm}

<!-- <br>\vspace{1cm} -->

<!-- ```{r scpmesim8, eval = T, echo = F, fig.cap = "Figure caption here."} -->

<!-- # shrinkB MSPE by lam and const (N = 100, P = 200, R = 10, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 100, P == 200, R == 10, sparse == 0.5, sigma == "compound", model == "shrinkB") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->

<!-- min = d[d$mean == min(d$mean),] -->

<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkB MSPE (N = 100, P = 200, R = 10, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->

<!-- ``` -->

<!-- ##### `shrinkBO` -->

<!-- <br>\vspace{1cm} -->

\vspace{0.5cm}
```{r scpmesim9, eval = T, echo = F, fig.cap = "The oracle precision matrices were tri-diagonal with dimension p = 200 and the data was generated with a sample size n = 100 and response vector dimension r = 10. The cross validation MSPE are plotted for each lambda and constant tuning parameter pair. The optimal tuning parameter pair was found to be log10(lam) = -0.5 and const = 0.3. Note that brighter areas signify smaller losses."}

# shrinkBO MSPE by lam and const (N = 100, P = 200, R = 10, sigma = "tridiag")
d = sigma %>% filter(N == 100, P == 200, R == 10, sparse == 0.5, sigma == "tridiag", model == "shrinkBO") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean)))

min = d[d$mean == min(d$mean),]

d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkBO Covariance Shrinkage", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = ""))

```
\vspace{0.5cm}


<!-- <br>\vspace{1cm} -->

<!-- ```{r scpmesim10, eval = T, echo = F, fig.cap = "Figure caption here."} -->

<!-- # shrinkBO MSPE by lam and const (N = 100, P = 200, R = 10, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 100, P == 200, R == 10, sparse == 0.5, sigma == "compound", model == "shrinkBO") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->

<!-- min = d[d$mean == min(d$mean),] -->

<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkBO MSPE (N = 100, P = 200, R = 10, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->

<!-- ``` -->

<!-- <br>\vspace{1cm} -->

<!-- #### Low Dimension -->

<!-- ##### `shrinkB` -->

<!-- <br>\vspace{1cm} -->

<!-- ```{r scpmesim11, eval = T, echo = F, fig.cap = "shrinkB MSPE (N = 1000, P = 50, R = 1, sigma = tridiag). Low dimension."} -->

<!-- # shrinkB MSPE by lam and const (N = 1000, P = 50, R = 1, sigma = "tridiag") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "tridiag", model == "shrinkB") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->

<!-- min = d[d$mean == min(d$mean),] -->

<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkB MSPE (N = 1000, P = 50, R = 1, sigma = tridiag)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->

<!-- ``` -->

<!-- <br>\vspace{1cm} -->

<!-- ```{r scpmesim12, eval = T, echo = F, fig.cap = "Figure caption here."} -->

<!-- # shrinkB MSPE by lam and const (N = 1000, P = 50, R = 1, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "compound", model == "shrinkB") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->

<!-- min = d[d$mean == min(d$mean),] -->

<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkB MSPE (N = 1000, P = 50, R = 1, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->

<!-- ``` -->

<!-- #### `shrinkBO` -->

<!-- <br>\vspace{1cm} -->

<!-- ```{r scpmesim13, eval = T, echo = F, fig.cap = "shrinkBO MSPE (N = 1000, P = 50, R = 1, sigma = tridiag). Low dimension."} -->

<!-- # shrinkBO MSPE by lam and const (N = 1000, P = 10, R = 1, sigma = "tridiag") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "tridiag", model == "shrinkBO") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->

<!-- min = d[d$mean == min(d$mean),] -->

<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkBO MSPE (N = 1000, P = 50, R = 1, sigma = tridiag)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->

<!-- ``` -->

<!-- <br>\vspace{1cm} -->

<!-- ```{r scpmesim14, eval = T, echo = F, fig.cap = "Figure caption here."} -->

<!-- # shrinkBO MSPE by lam and const (N = 1000, P = 50, R = 1, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "compound", model == "shrinkBO") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->

<!-- min = d[d$mean == min(d$mean),] -->

<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkBO MSPE (N = 1000, P = 50, R = 1, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->

<!-- ``` -->

<!-- <br>\vspace{1cm} -->

\newpage

## Discussion

Apart from the two SCPME estimators discussed in the previous section, the generality of the SCPME framework allows for many previously unknown precision matrix estimators to be explored. One estimator that was concieved of in our work but deserves further attention in future research is of the form

\begin{equation}
\hat{\Omega}_{x} = \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr(S_{x}\Omega_{x}) - \log\left| \Omega_{x} \right| + \frac{\lambda}{2}\left\| \mathbb{X}\Omega_{x} \Sigma_{xy} - \mathbb{Y} \right\|_{F}^{2} \right\}
(\#eq:penregression)\notag
\end{equation}

Note that here we are using the Frobenius norm instead of the matrix $l_{1}$-norm in the SCPME framework but this requires only a slight modification of the augmented ADMM algorithm - which will be presented later. As before, the script notation denotes matrices that are column-centered. This optimization problem and objective function estimates a precision matrix that balances minimizing the gaussian negative log-likelihood for $\Omega_{x}$ with minimizing the MSPE for the forward regression coefficient matrix $\beta \equiv \Omega_{x}\Sigma_{xy}$. In other words, this estimator estimates a precision matrix using the likelihood function but that does not otherwise hinder predictive performance of the forward regression model.

This estimator also reveals an interesting connection to the joint log-likelihood of $X \in \mathbb{R}^{p}$ and $Y \in \mathbb{R}^{r}$ (assuming $X$ is now random). To see this, let us suppose that we have $n$ independent copies of the random pair $(Y_{i}, X_{i})$ and we assume a linear relationship.

\begin{equation}
Y_{i} = \mu_{y} + \beta'\left(X_{i} - \mu_{x}\right) + E_{i}
\end{equation}

where $E_{i} \sim N_{r}\left( 0, \Omega_{y | x}^{-1} \right)$ and $X_{i} \sim N_{p}\left( \mu_{x}, \Omega_{x}^{-1} \right)$. This implies that the conditional distribution of $Y_{i}|X_{i}$ is of the form

\begin{equation}
Y_{i} | X_{i} \sim N_{r}\left( \mu_{y} + \beta'\left(X_{i} - \mu_{x}\right), \Omega_{y | x}^{-1} \right)
\end{equation}

We can use this conditional distribution along with the marginal distribution of $X$ to derive the joint log-likelihood of $X$ and $Y$. Without loss of generality, we will assume that $\mu_{x} = \mu_{y} = 0$. This can similarly achieved by simply centering the data by the sample averages.

\begin{equation}
\begin{split}
  l\left( \Omega_{y | x}, \Omega_{x}, \Sigma_{xy} | Y, X \right) &= constant + \frac{n}{2}\log\left| \Omega_{y | x} \right| - \frac{1}{2}\sum_{i = 1}^{n} tr\left[ \left( Y_{i} - \Sigma_{xy}'\Omega_{x} X_{i} \right)\left( Y_{i} - \Sigma_{xy}'\Omega_{x} X_{i} \right)'\Omega_{y | x} \right] \\
  &+\frac{n}{2}\log\left| \Omega \right| - \frac{1}{2}\sum_{i = 1}^{n}tr\left( X_{i}X_{i}'\Omega_{x} \right) \\
  &= constant + \frac{n}{2}\log\left| \Omega_{y | x} \right| - \frac{1}{2}tr\left[ \left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{xy} \right)'\left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{y | x} \right)\Omega_{y | x} \right] \\
  &+ \frac{n}{2}\log\left| \Omega_{x} \right| - \frac{n}{2}tr\left( S_{x}\Omega_{x} \right)
(\#eq:partialjointloglik)\notag
\end{split}
\end{equation}

Note that $\beta \equiv \Omega\Sigma_{xy}$ and $S_{x}$ still denotes the marginal sample covariance matrix of random variable $X$. Now suppose we are interested in optimizing this gaussian log-likelihood function with respect to $\Omega$:

\begin{equation}
\begin{split}
  \hat{\Omega}_{x} &= \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ \frac{1}{2}tr\left[ \left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{xy} \right)^{T}\left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{xy} \right)\Omega_{y | x} \right] + \frac{n}{2}tr\left( S_{x}\Omega_{x} \right) - \frac{n}{2}\log\left| \Omega_{x} \right| \right\} \\
  &= \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr\left( S_{x}\Omega_{x} \right) - \log\left| \Omega_{x} \right| + \frac{1}{n}\left\| \left( \mathbb{X}\Omega_{x}\Sigma_{xy} - \mathbb{Y} \right)\Omega_{y | x}^{1/2} \right\|_{F}^{2} \right\}
(\#eq:omegajointloglik)\notag
\end{split}
\end{equation}

If we assume that, given $X$, each of the $r$ responses are pairwise independent with equal variance so that $\Omega_{y | x}^{1/2} = I_{r}/\sigma_{y | x}$ and let $\lambda = 2/(n\sigma_{y | x}^{2})$, then we have that

\begin{equation}
\hat{\Omega}_{x} = \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr\left( S_{x}\Omega_{x} \right) - \log\left| \Omega \right| + \frac{\lambda}{2}\left\| \mathbb{X}\Omega\Sigma_{xy} - \mathbb{Y} \right\|_{F}^{2} \right\}
(\#eq:omegajointmle)\notag
\end{equation}

There are a few takeaways we can make after revealing this connection. The first is that the estimator constructed in the previous section is, in fact, solving a very similar problem to that of optimizing the joint likelihood with respect to $\Omega$.


Note that the only difference is in step 4.

```{theorem, name = "Modified Augmented ADMM Algorithm for Shrinking Characteristics of Precision Matrix Estimators with Frobenius Norm"}

Set $k = 0$ and initialize $Z^{0}, \Lambda^{0}, \Omega^{0}$, and $\rho$ and repeat steps 1-5 until convergence.

1. Compute $G^{k} = \rho A'\left( A\Omega^{k} B - Z^{k} - C + \rho^{-1}\Lambda^{k} \right)B'$

2. Decompose $S + \left( G^{k} + (G^{k})' \right)/2 - \rho\tau\Omega^{k} = VQV'$ (via the spectral decomposition).

3. Set $\Omega^{k + 1} = V\left( -Q + (Q^{2} + 4\rho\tau I_{p})^{1/2} \right)V'/(2\rho\tau)$

4. Set $Z^{k + 1} = \left[ \rho\left( A\Omega^{k + 1} B - C \right) + \Lambda^{k} \right]/(\lambda + \rho)$

5. Set $\Lambda^{k + 1} = \rho\left( A\Omega^{k + 1} B - Z^{k + 1} - C \right)$

```

Note that no closed-form solution to this estimator exists and so we must resort to some iterative algorithm similar to the SCPME augmented ADMM algorithm in order to solve it^[Proof in section \@ref(proofpenregression).].
