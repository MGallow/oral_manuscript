[
["index.html", "Oral Manuscript Chapter 1 Prerequisites", " Oral Manuscript Matt Galloway 2018-06-20 Chapter 1 Prerequisites "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction This is the introduction for Matt’s oral manuscript. (UPDATED 8) "],
["tutorial.html", "Chapter 3 Tutorial 3.1 Usage", " Chapter 3 Tutorial # The easiest way to install is from CRAN install.packages(&quot;ADMMsigma&quot;) # You can also install the development version from GitHub: # install.packages(&#39;devtools&#39;) devtools::install_github(&quot;MGallow/ADMMsigma&quot;) If there are any issues/bugs, please let me know: github. You can also contact me via my website. Pull requests are welcome! A (possibly incomplete) list of functions contained in the package can be found below: ADMMsigma() computes the estimated precision matrix (ridge, lasso, and elastic-net type regularization optional) RIDGEsigma() computes the estimated ridge penalized precision matrix via closed-form solution plot.ADMMsigma() produces a heat map or line graph for cross validation errors plot.RIDGEsigma() produces a heat map or line graph for cross validation errors 3.1 Usage We will first generate data from a sparse, tri-diagonal precision matrix and denote it as Omega. library(ADMMsigma) # generate data from a sparse matrix first compute covariance matrix S = matrix(0.7, nrow = 5, ncol = 5) for (i in 1:5) { for (j in 1:5) { S[i, j] = S[i, j]^abs(i - j) } } # print oracle precision matrix (shrinkage might be useful) (Omega = round(qr.solve(S), 3)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.961 -1.373 0.000 0.000 0.000 ## [2,] -1.373 2.922 -1.373 0.000 0.000 ## [3,] 0.000 -1.373 2.922 -1.373 0.000 ## [4,] 0.000 0.000 -1.373 2.922 -1.373 ## [5,] 0.000 0.000 0.000 -1.373 1.961 # generate 100 x 5 matrix with rows drawn from iid N_p(0, S) set.seed(123) Z = matrix(rnorm(100 * 5), nrow = 100, ncol = 5) out = eigen(S, symmetric = TRUE) S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors) X = Z %*% S.sqrt # snap shot of data head(X) ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.4311177 -0.217744186 1.276826576 -0.1061308 -0.02363953 ## [2,] -0.0418538 0.304253474 0.688201742 -0.5976510 -1.06758924 ## [3,] 1.1344174 0.004493877 -0.440059159 -0.9793198 -0.86953222 ## [4,] -0.0738241 -0.286438212 0.009577281 -0.7850619 -0.32351261 ## [5,] -0.2905499 -0.906939891 -0.656034183 -0.4324413 0.28516534 ## [6,] 1.3761967 0.276942730 -0.297518545 -0.2634814 -1.35944340 As described earlier in the report, the maximum likelihood estimator (MLE) for Omega is the inverse of the sample precision matrix \\(S^{-1} = \\left[\\sum_{i = 1}^{n}(X_{i} - \\bar{X})(X_{i} - \\bar{X})^{T}/n \\right]^{-1}\\): # print inverse of sample precision matrix (perhaps a bad estimate) round(qr.solve(cov(X) * (nrow(X) - 1)/nrow(X)), 5) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.32976 -1.55033 0.22105 -0.08607 0.24309 ## [2,] -1.55033 3.27561 -1.68026 -0.14277 0.18949 ## [3,] 0.22105 -1.68026 3.19897 -1.25158 -0.11016 ## [4,] -0.08607 -0.14277 -1.25158 2.76790 -1.37226 ## [5,] 0.24309 0.18949 -0.11016 -1.37226 2.05377 However, because Omega (known as the oracle) is sparse, a shrinkage estimator will perhaps perform better than the sample estimator. Below we construct various penalized estimators: # elastic-net type penalty (set tolerance to 1e-8) ADMMsigma(X, tol.abs = 1e-08, tol.rel = 1e-08) ## ## Call: ADMMsigma(X = X, tol.abs = 1e-08, tol.rel = 1e-08) ## ## Iterations: 162 ## ## Tuning parameters: ## log10(lam) alpha ## [1,] -1.599 1 ## ## Log-likelihood: -108.41003 ## ## Omega: ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.15283 -1.26902 0.00000 0.00000 0.19765 ## [2,] -1.26902 2.79032 -1.32206 -0.08056 0.00925 ## [3,] 0.00000 -1.32206 2.85470 -1.17072 -0.00865 ## [4,] 0.00000 -0.08056 -1.17072 2.49554 -1.18959 ## [5,] 0.19765 0.00925 -0.00865 -1.18959 1.88121 LASSO: # lasso penalty (default tolerance) ADMMsigma(X, alpha = 1) ## ## Call: ADMMsigma(X = X, alpha = 1) ## ## Iterations: 66 ## ## Tuning parameters: ## log10(lam) alpha ## [1,] -1.599 1 ## ## Log-likelihood: -108.41022 ## ## Omega: ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.15228 -1.26841 0.00000 0.00000 0.19744 ## [2,] -1.26841 2.78830 -1.31943 -0.08246 0.01018 ## [3,] 0.00000 -1.31943 2.84979 -1.16708 -0.01015 ## [4,] 0.00000 -0.08246 -1.16708 2.49277 -1.18844 ## [5,] 0.19744 0.01018 -0.01015 -1.18844 1.88069 ELASTIC-NET: # elastic-net penalty (alpha = 0.5) ADMMsigma(X, alpha = 0.5) ## ## Call: ADMMsigma(X = X, alpha = 0.5) ## ## Iterations: 67 ## ## Tuning parameters: ## log10(lam) alpha ## [1,] -1.821 0.5 ## ## Log-likelihood: -101.13595 ## ## Omega: ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.20031 -1.32471 0.01656 -0.00334 0.21798 ## [2,] -1.32471 2.90659 -1.37599 -0.19084 0.13651 ## [3,] 0.01656 -1.37599 2.92489 -1.12859 -0.12033 ## [4,] -0.00334 -0.19084 -1.12859 2.56559 -1.23472 ## [5,] 0.21798 0.13651 -0.12033 -1.23472 1.94528 RIDGE: # ridge penalty ADMMsigma(X, alpha = 0) ## ## Call: ADMMsigma(X = X, alpha = 0) ## ## Iterations: 65 ## ## Tuning parameters: ## log10(lam) alpha ## [1,] -1.821 0 ## ## Log-likelihood: -99.19746 ## ## Omega: ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.18979 -1.31533 0.04515 -0.04090 0.23511 ## [2,] -1.31533 2.90019 -1.37049 -0.22633 0.17808 ## [3,] 0.04515 -1.37049 2.89435 -1.07647 -0.17369 ## [4,] -0.04090 -0.22633 -1.07647 2.55026 -1.22786 ## [5,] 0.23511 0.17808 -0.17369 -1.22786 1.95495 # ridge penalty no ADMM RIDGEsigma(X, lam = 10^seq(-8, 8, 0.01)) ## ## Call: RIDGEsigma(X = X, lam = 10^seq(-8, 8, 0.01)) ## ## Tuning parameter: ## log10(lam) lam ## [1,] -2.17 0.007 ## ## Log-likelihood: -109.18156 ## ## Omega: ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.15416 -1.31185 0.08499 -0.05571 0.22862 ## [2,] -1.31185 2.85605 -1.36677 -0.19650 0.16880 ## [3,] 0.08499 -1.36677 2.82606 -1.06325 -0.14946 ## [4,] -0.05571 -0.19650 -1.06325 2.50721 -1.21935 ## [5,] 0.22862 0.16880 -0.14946 -1.21935 1.92871 This package also has the capability to provide heat maps for the cross validation errors. The more bright (white) areas of the heat map pertain to more optimal tuning parameters. # produce CV heat map for ADMMsigma ADMM = ADMMsigma(X, tol.abs = 1e-08, tol.rel = 1e-08) plot(ADMM, type = &quot;heatmap&quot;) # produce line graph for CV errors for ADMMsigma plot(ADMM, type = &quot;line&quot;) # produce CV heat map for RIDGEsigma RIDGE = RIDGEsigma(X, lam = 10^seq(-8, 8, 0.01)) plot(RIDGE, type = &quot;heatmap&quot;) # produce line graph for CV errors for RIDGEsigma plot(RIDGE, type = &quot;line&quot;) "],
["details.html", "Chapter 4 Details 4.1 ADMM Algorithm 4.2 Scaled-Form ADMM 4.3 Algorithm", " Chapter 4 Details Suppose we want to solve the following optimization problem: where \\(x \\in \\mathbb{R}^{n}, z \\in \\mathbb{R}^{m}, A \\in \\mathbb{R}^{p \\times n}, B \\in \\mathbb{R}^{p \\times m}\\), and \\(c \\in \\mathbb{R}^{p}\\). Following Boyd et al. (2011), the optimization problem will be introduced in vector form though we will later consider cases where \\(x\\) and \\(z\\) are matrices. We will also assume \\(f\\) and \\(g\\) are convex functions. Optimization problems like this arise naturally in several statistics and machine learning applications – particularly regularization methods. For instance, we could take \\(f\\) to be the squared error loss, \\(g\\) to be the \\(l_{2}\\)-norm, \\(c\\) to be equal to zero and \\(A\\) and \\(B\\) to be identity matrices to solve the ridge regression optimization problem. The augmented lagrangian is constructed as follows: \\[ L_{\\rho}(x, z, y) = f(x) + g(z) + y^{T}(Ax + Bz - c) + \\frac{\\rho}{2}\\left\\| Ax + Bz - c \\right\\|_{2}^{2} \\] where \\(y \\in \\mathbb{R}^{p}\\) is the lagrange multiplier and \\(\\rho &gt; 0\\) is a scalar. Clearly any minimizer, \\(p^{*}\\), under the augmented lagrangian is equivalent to that of the lagrangian since any feasible point \\((x, z)\\) satisfies the constraint \\(\\rho\\left\\| Ax + Bz - c \\right\\|_{2}^{2}/2 = 0\\). \\[ p^{*} = inf\\left\\{ f(x) + g(z) | Ax + Bz = c \\right\\} \\] The alternating direction method of multipliers (ADMM) algorithm consists of the following repeated iterations: A more complete introduction to the algorithm – specifically how it arose out of dual ascent and method of multipliers – can be found in Boyd et al. (2011). 4.1 ADMM Algorithm Now consider the case where \\(X_{1}, ..., X_{n}\\) are iid \\(N_{p}(\\mu, \\Sigma)\\) random variables and we are tasked with estimating the precision matrix, denoted \\(\\Omega \\equiv \\Sigma^{-1}\\). The maximum likelihood estimator for \\(\\Omega\\) is \\[ \\hat{\\Omega}_{MLE} = \\arg\\min_{\\Omega \\in S_{+}^{p}}\\left\\{ Tr\\left(S\\Omega\\right) - \\log \\det\\left(\\Omega \\right) \\right\\} \\] where \\(S = \\sum_{i = 1}^{n}(X_{i} - \\bar{X})(X_{i} - \\bar{X})^{T}/n\\) and \\(\\bar{X}\\) is the sample mean. By setting the gradient equal to zero, we can show that when the solution exists, \\(\\hat{\\Omega}_{MLE} = S^{-1}\\). As in regression settings, we can construct a penalized likelihood estimator by adding a penalty term, \\(P\\left( \\Omega \\right)\\), to the likelihood: \\[ \\hat{\\Omega} = \\arg\\min_{\\Omega \\in S_{+}^{p}}\\left\\{ Tr\\left(S\\Omega\\right) - \\log \\det\\left(\\Omega \\right) + P\\left( \\Omega \\right) \\right\\} \\] \\(P\\left( \\Omega \\right)\\) is often of the form \\(P\\left(\\Omega \\right) = \\lambda\\|\\Omega \\|_{F}^{2}\\) or \\(P\\left(\\Omega \\right) = \\|\\Omega\\|_{1}\\) where \\(\\lambda &gt; 0\\), \\(\\left\\|\\cdot \\right\\|_{F}^{2}\\) is the Frobenius norm and we define \\(\\left\\|A \\right\\|_{1} = \\sum_{i, j} \\left| A_{ij} \\right|\\). These penalties are the ridge and lasso, respectively. We will, instead, take \\(P\\left( \\Omega \\right) = \\lambda\\left[\\frac{1 - \\alpha}{2}\\left\\| \\Omega \\right|_{F}^{2} + \\alpha\\left\\| \\Omega \\right\\|_{1} \\right]\\) so that the full penalized likelihood is \\[ \\hat{\\Omega} = \\arg\\min_{\\Omega \\in S_{+}^{p}}\\left\\{ Tr\\left(S\\Omega\\right) - \\log \\det\\left(\\Omega \\right) + \\lambda\\left[\\frac{1 - \\alpha}{2}\\left\\| \\Omega \\right|_{F}^{2} + \\alpha\\left\\| \\Omega \\right\\|_{1} \\right] \\right\\} \\] where \\(0 \\leq \\alpha \\leq 1\\). This elastic-net penalty was explored by Hui Zou and Trevor Hastie (Zou and Hastie 2005) and is identical to the penalty used in the popular penalized regression package glmnet. Clearly, when \\(\\alpha = 0\\) the elastic-net reduces to a ridge-type penalty and when \\(\\alpha = 1\\) it reduces to a lasso-type penalty. By letting \\(f\\) be equal to the non-penalized likelihood and \\(g\\) equal to \\(P\\left( \\Omega \\right)\\), our goal is to minimize the full augmented lagrangian where the constraint is that \\(\\Omega - Z\\) is equal to zero: \\[ L_{\\rho}(\\Omega, Z, \\Lambda) = f\\left(\\Omega\\right) + g\\left(Z\\right) + Tr\\left[\\Lambda\\left(\\Omega - Z\\right)\\right] + \\frac{\\rho}{2}\\left\\|\\Omega - Z\\right\\|_{F}^{2} \\] The ADMM algorithm for estimating the penalized precision matrix in this problem is 4.2 Scaled-Form ADMM An alternate form of the ADMM algorithm can constructed by scaling the dual variable (\\(\\Lambda^{k}\\)). Let us define \\(R^{k} = \\Omega - Z^{k}\\) and \\(U^{k} = \\Lambda^{k}/\\rho\\). Therefore, the condensed-form ADMM algorithm can now be written as And more generally (in vector form), Note that there are limitations to using this method. Because the dual variable is scaled by \\(\\rho\\) (the step size), this form limits one to using a constant step size without making further adjustments to \\(U^{k}\\). It has been shown in the literature that a dynamic step size can significantly reduce the number of iterations required for convergence. 4.3 Algorithm Initialize \\(Z^{0}, \\Lambda^{0}\\), and \\(\\rho\\). Iterate the following three steps until convergence: Decompose \\(S + \\Lambda^{k} - \\rho Z^{k} = VQV^{T}\\) via spectral decomposition. \\[ \\Omega^{k + 1} = \\frac{1}{2\\rho}V\\left[ -Q + \\left( Q^{2} + 4\\rho I_{p} \\right)^{1/2} \\right]V^{T} \\] Elementwise soft-thresholding for all \\(i = 1,..., p\\) and \\(j = 1,..., p\\). Update \\(\\Lambda^{k + 1}\\). \\[ \\Lambda^{k + 1} = \\Lambda^{k} + \\rho\\left( \\Omega^{k + 1} - Z^{k + 1} \\right) \\] 4.3.1 Proof of (1): \\[ \\Omega^{k + 1} = \\arg\\min_{\\Omega}\\left\\{ Tr\\left(S\\Omega\\right) - \\log\\det\\left(\\Omega\\right) + Tr\\left[\\Lambda^{k}\\left(\\Omega - Z^{k}\\right)\\right] + \\frac{\\rho}{2}\\left\\| \\Omega - Z^{k} \\right\\|_{F}^{2} \\right\\} \\] Set the gradient equal to zero and decompose \\(\\Omega = VDV^{T}\\) where \\(D\\) is a diagonal matrix with diagonal elements equal to the eigen values of \\(\\Omega\\) and \\(V\\) is the matrix with corresponding eigen vectors as columns. \\[ S + \\Lambda^{k} - \\rho Z^{k} = \\Omega^{-1} - \\rho \\Omega = VD^{-1}V^{T} - \\rho VDV^{T} = V\\left(D^{-1} - \\rho D\\right)V^{T} \\] This equivalence implies that \\[ \\phi_{j}\\left( S + \\Lambda^{k} - \\rho Z^{k} \\right) = \\frac{1}{\\phi_{j}(\\Omega^{k + 1})} - \\rho\\phi_{j}(\\Omega^{k + 1}) \\] where \\(\\phi_{j}(\\cdot)\\) is the \\(j\\)th eigen value. In summary, if we decompose \\(S + \\Lambda^{k} - \\rho Z^{k} = VQV^{T}\\) then \\[ \\Omega^{k + 1} = \\frac{1}{2\\rho}V\\left[ -Q + (Q^{2} + 4\\rho I_{p})^{1/2}\\right] V^{T} \\] 4.3.2 Proof of (2) \\[ Z^{k + 1} = \\arg\\min_{Z}\\left\\{ \\lambda\\left[ \\frac{1 - \\alpha}{2}\\left\\| Z \\right\\|_{F}^{2} + \\alpha\\left\\| Z \\right\\|_{1} \\right] + Tr\\left[\\Lambda^{k}\\left(\\Omega^{k + 1} - Z\\right)\\right] + \\frac{\\rho}{2}\\left\\| \\Omega^{k + 1} - Z \\right\\|_{F}^{2} \\right\\} \\] where \\(Sign(Z)\\) is the elementwise Sign operator. By setting the gradient/sub-differential equal to zero, we arrive at the following equivalence: \\[ Z_{ij} = \\frac{1}{\\lambda(1 - \\alpha) + \\rho}\\left( \\rho \\Omega_{ij}^{k + 1} + \\Lambda_{ij}^{k} - Sign(Z_{ij})\\lambda\\alpha \\right) \\] for all \\(i = 1,..., p\\) and \\(j = 1,..., p\\). We observe two scenarios: If \\(Z_{ij} &gt; 0\\) then \\[ \\rho\\Omega_{ij}^{k + 1} + \\Lambda_{ij}^{k} &gt; \\lambda\\alpha \\] If \\(Z_{ij} &lt; 0\\) then \\[ \\rho\\Omega_{ij}^{k + 1} + \\Lambda_{ij}^{k} &lt; -\\lambda\\alpha \\] This implies that \\(Sign(Z_{ij}) = Sign(\\rho\\Omega_{ij}^{k + 1} + \\Lambda_{ij}^{k})\\). Putting all the pieces together, we arrive at where \\(Soft\\) is the soft-thresholding function. References "],
["simulations.html", "Chapter 5 Simulations 5.1 Compound Symmetric: P = 100, N = 50 5.2 Compound Symmetric: P = 10, N = 1000 5.3 Dense: P = 100, N = 50 5.4 Dense: P = 10, N = 50 5.5 Tridiagonal: P = 100, N = 50", " Chapter 5 Simulations In the simulations below we generated data from a number of oracle precision matrices with various structures. For each data-generating procedure, the ADMMsigma() function was run using 5-fold cross validation. After 20 replications, the cross validation errors were totalled and the optimal tuning parameters were selected (results in the top figure). These results are compared with the Kullback Leibler (KL) losses between the estimates and the oracle precision matrix (bottom figure). We can see below that our cross validation procedure choosing tuning parameters close to the optimal parameters. 5.1 Compound Symmetric: P = 100, N = 50 # oracle precision matrix Omega = matrix(0.9, ncol = 100, nrow = 100) diag(Omega = 1) # generate covariance matrix S = qr.solve(Omega) # generate data Z = matrix(rnorm(100 * 50), nrow = 50, ncol = 100) out = eigen(S, symmetric = TRUE) S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors) X = Z %*% S.sqrt 5.2 Compound Symmetric: P = 10, N = 1000 # oracle precision matrix Omega = matrix(0.9, ncol = 10, nrow = 10) diag(Omega = 1) # generate covariance matrix S = qr.solve(Omega) # generate data Z = matrix(rnorm(10 * 1000), nrow = 1000, ncol = 10) out = eigen(S, symmetric = TRUE) S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors) X = Z %*% S.sqrt 5.3 Dense: P = 100, N = 50 # generate eigen values eigen = c(rep(1000, 5, rep(1, 100 - 5))) # randomly generate orthogonal basis (via QR) Q = matrix(rnorm(100*100), nrow = 100, ncol = 100) %&gt;% qr %&gt;% qr.Q # generate covariance matrix S = Q %*% diag(eigen) %*% t(Q) # generate data Z = matrix(rnorm(100*50), nrow = 50, ncol = 100) out = eigen(S, symmetric = TRUE) S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors) X = Z %*% S.sqrt 5.4 Dense: P = 10, N = 50 # generate eigen values eigen = c(rep(1000, 5, rep(1, 10 - 5))) # randomly generate orthogonal basis (via QR) Q = matrix(rnorm(10*10), nrow = 10, ncol = 10) %&gt;% qr %&gt;% qr.Q # generate covariance matrix S = Q %*% diag(eigen) %*% t(Q) # generate data Z = matrix(rnorm(10*50), nrow = 50, ncol = 10) out = eigen(S, symmetric = TRUE) S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors) X = Z %*% S.sqrt 5.5 Tridiagonal: P = 100, N = 50 # generate covariance matrix # (can confirm inverse is tri-diagonal) S = matrix(0, nrow = 100, ncol = 100) for (i in 1:100){ for (j in 1:100){ S[i, j] = 0.7^abs(i - j) } } # generate data Z = matrix(rnorm(10*50), nrow = 50, ncol = 10) out = eigen(S, symmetric = TRUE) S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors) X = Z %*% S.sqrt "],
["benchmark.html", "Chapter 6 Benchmark", " Chapter 6 Benchmark Below we benchmark the various functions contained in ADMMsigma. We can see that ADMMsigma (at the default tolerance) offers comparable computation time to the popular glasso R package. 6.0.1 Computer Specs: MacBook Pro (Late 2016) Processor: 2.9 GHz Intel Core i5 Memory: 8GB 2133 MHz Graphics: Intel Iris Graphics 550 library(ADMMsigma) library(microbenchmark) # generate data from tri-diagonal (sparse) matrix compute covariance matrix # (can confirm inverse is tri-diagonal) S = matrix(0, nrow = 100, ncol = 100) for (i in 1:100) { for (j in 1:100) { S[i, j] = 0.7^(abs(i - j)) } } # generate 1000 x 100 matrix with rows drawn from iid N_p(0, S) set.seed(123) Z = matrix(rnorm(1000 * 100), nrow = 1000, ncol = 100) out = eigen(S, symmetric = TRUE) S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors) X = Z %*% S.sqrt # glasso (for comparison) microbenchmark(glasso::glasso(s = S, rho = 0.1)) ## Unit: milliseconds ## expr min lq mean median ## glasso::glasso(s = S, rho = 0.1) 51.69035 54.94714 64.29637 57.50123 ## uq max neval ## 63.6842 176.9435 100 # benchmark ADMMsigma - default tolerance microbenchmark(ADMMsigma(S = S, lam = 0.1, alpha = 1, tol.abs = 1e-04, tol.rel = 1e-04, trace = &quot;none&quot;)) ## Unit: milliseconds ## expr ## ADMMsigma(S = S, lam = 0.1, alpha = 1, tol.abs = 1e-04, tol.rel = 1e-04, trace = &quot;none&quot;) ## min lq mean median uq max neval ## 80.99452 83.64047 92.47847 87.70749 99.56716 127.6635 100 # benchmark ADMMsigma - tolerance 1e-8 microbenchmark(ADMMsigma(S = S, lam = 0.1, alpha = 1, tol.abs = 1e-08, tol.rel = 1e-08, trace = &quot;none&quot;)) ## Unit: milliseconds ## expr ## ADMMsigma(S = S, lam = 0.1, alpha = 1, tol.abs = 1e-08, tol.rel = 1e-08, trace = &quot;none&quot;) ## min lq mean median uq max neval ## 258.8267 269.8397 288.8802 278.3885 295.6218 456.7028 100 # benchmark ADMMsigma CV - default parameter grid microbenchmark(ADMMsigma(X, trace = &quot;none&quot;), times = 5) ## Unit: seconds ## expr min lq mean median uq ## ADMMsigma(X, trace = &quot;none&quot;) 8.173835 8.216599 8.61319 8.228365 8.951196 ## max neval ## 9.495953 5 # benchmark ADMMsigma parallel CV microbenchmark(ADMMsigma(X, cores = 3, trace = &quot;none&quot;), times = 5) ## Unit: seconds ## expr min lq mean ## ADMMsigma(X, cores = 3, trace = &quot;none&quot;) 8.327664 9.460961 9.794332 ## median uq max neval ## 9.528513 10.46933 11.18519 5 # benchmark ADMMsigma CV - likelihood convergence criteria microbenchmark(ADMMsigma(X, crit = &quot;loglik&quot;, trace = &quot;none&quot;), times = 5) ## Unit: seconds ## expr min lq mean ## ADMMsigma(X, crit = &quot;loglik&quot;, trace = &quot;none&quot;) 7.107775 7.126597 7.172196 ## median uq max neval ## 7.143074 7.20464 7.278895 5 # benchmark RIDGEsigma CV microbenchmark(RIDGEsigma(X, lam = 10^seq(-8, 8, 0.01), trace = &quot;none&quot;), times = 5) ## Unit: seconds ## expr min ## RIDGEsigma(X, lam = 10^seq(-8, 8, 0.01), trace = &quot;none&quot;) 12.03989 ## lq mean median uq max neval ## 12.05414 12.18442 12.13432 12.18751 12.50624 5 "],
["penalized-regression-estimators.html", "Chapter 7 Penalized Regression Estimators 7.1 Formulas 7.2 Estimators", " Chapter 7 Penalized Regression Estimators This document explores a number of regression estimators subject to various assumptions. We will assume \\(n\\) samples of \\[ Y_{i} = \\mu_{y} + \\beta^{T}(X_{i} - \\mu_{x}) + \\epsilon_{i} \\] where \\(\\epsilon_{i} \\sim N_{r}\\left(0, \\Sigma_{y | x} \\right)\\) and \\(X_{i} \\sim N_{p}\\left(0, \\Sigma_{x}\\right)\\) so that by taking \\(\\theta = \\left( \\mu_{y}, \\mu_{x}, vec\\left( \\beta \\right), vec\\left( \\Sigma_{y | x}^{-1} \\right), vec\\left( \\Sigma_{xx}^{-1} \\right) \\right)^{T}\\) the joint log-likelihood is of the following form: For convenience, we will later denote \\(\\mathbb{X}\\) as the \\(n \\times p\\) matrix with rows \\(X_{i} - \\bar{X}\\) where \\(\\bar{X} = \\sum_{i = 1}^{n}X_{i}/n\\) and \\(\\mathbb{Y}\\) as the \\(n \\times r\\) matrix with rows \\(Y_{i} - \\bar{Y}\\) and \\(\\bar{Y}\\) defined similarly. Note that \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the maximum likelihood esitmators of \\(\\mu_{x}\\) and \\(\\mu_{y}\\), respectively. 7.1 Formulas Below we outline a few important formulas that will be used throughout this report: Forward regression coefficient: \\(\\beta = \\Sigma_{xx}^{-1}\\Sigma_{xy} = \\Sigma_{x | y}^{-1}\\alpha^{T}\\left( \\Sigma_{yy}^{-1} + \\alpha\\Sigma_{x | y}^{-1}\\alpha^{T} \\right)^{-1}\\) Inverse regression coefficient: \\(\\alpha = \\Sigma_{yy}^{-1}\\Sigma_{xy}^{T}\\) \\(\\Sigma_{y | x} = \\Sigma_{yy} - \\beta^{T}\\Sigma_{xx}\\beta = \\Sigma_{yy} - \\Sigma_{xy}^{T}\\Sigma_{xx}^{-1}\\Sigma_{xy}\\) \\(\\Sigma_{y | x}^{-1} = \\Sigma_{yy}^{-1} + \\Sigma_{y | x}^{-1}\\Sigma_{xy}^{T}\\left( \\Sigma_{yy} + \\Sigma_{xy}\\Sigma_{y | x}^{-1}\\Sigma_{xy}^{T} \\right)^{-1}\\Sigma_{xy}\\Sigma_{y | x}^{-1} = \\Sigma_{yy}^{-1} + \\Sigma_{y | x}^{-1}\\beta\\left( \\Sigma_{xx}^{-1} + \\beta\\Sigma_{y | x}^{-1}\\beta^{T} \\right)^{-1}\\beta\\Sigma_{y | x}^{-1}\\) \\(\\Sigma_{x | y} = \\Sigma_{xx} - \\alpha^{T}\\Sigma_{yy}\\alpha = \\Sigma_{xx} - \\Sigma_{xy}\\Sigma_{yy}^{-1}\\Sigma_{xy}^{T}\\) \\(\\Sigma_{x | y}^{-1} = \\Sigma_{xx}^{-1} + \\Sigma_{x | y}^{-1}\\Sigma_{xy}\\left( \\Sigma_{yy} + \\Sigma_{xy}^{T}\\Sigma_{x | y}^{-1}\\Sigma_{xy} \\right)^{-1}\\Sigma_{xy}^{T}\\Sigma_{x | y}^{-1} = \\Sigma_{xx}^{-1} + \\Sigma_{x | y}^{-1}\\alpha\\left( \\Sigma_{yy}^{-1} + \\alpha\\Sigma_{x | y}^{-1}\\alpha^{T} \\right)^{-1}\\alpha\\Sigma_{x | y}^{-1}\\) \\(\\Sigma_{yy}^{-1} = \\Sigma_{y | x}^{-1} - \\Sigma_{y | x}^{-1}\\Sigma_{xy}^{T}\\left( \\Sigma_{xx} + \\Sigma_{xy}\\Sigma_{y | x}^{-1}\\Sigma_{xy}^{T} \\right)^{-1}\\Sigma_{xy}\\Sigma_{y | x}^{-1} = \\Sigma_{y | x}^{-1} - \\Sigma_{y | x}^{-1}\\beta\\left( \\Sigma_{xx}^{-1} + \\beta\\Sigma_{y | x}^{-1}\\beta^{T} \\right)^{-1}\\beta\\Sigma_{y | x}^{-1}\\) \\(\\Sigma_{xx}^{-1} = \\Sigma_{x | y}^{-1} - \\Sigma_{x | y}^{-1}\\Sigma_{xy}\\left( \\Sigma_{yy} + \\Sigma_{xy}^{T}\\Sigma_{x | y}^{-1}\\Sigma_{xy} \\right)^{-1}\\Sigma_{xy}^{T}\\Sigma_{x | y}^{-1} = \\Sigma_{x | y}^{-1} - \\Sigma_{x | y}^{-1}\\alpha\\left( \\Sigma_{yy}^{-1} + \\alpha\\Sigma_{x | y}^{-1}\\alpha^{T} \\right)^{-1}\\alpha\\Sigma_{x | y}^{-1}\\) 7.2 Estimators 7.2.1 Assume ridge \\(\\beta\\), optimize in-sample prediction error \\[ \\hat{\\beta} = \\arg\\min_{\\beta}\\left\\{\\frac{1}{2} \\left\\| \\mathbb{Y} - \\mathbb{X}\\beta \\right\\|_{F}^{2} + \\frac{\\lambda}{2}\\left\\| \\beta \\right\\|_{F}^{2}\\right\\} \\] 7.2.2 Assume ridge \\(\\beta\\), optimize the likelihood for \\(\\beta\\) \\[ \\hat{\\beta} = \\arg\\min_{\\beta}\\left\\{ \\frac{1}{n} Tr\\left[ \\left(\\mathbb{Y} - \\mathbb{X}\\beta \\right)^{T}\\left( \\mathbb{Y} - \\mathbb{X}\\beta \\right)\\Sigma_{y | x}^{-1} \\right] - \\frac{1}{2}log\\left| \\Sigma_{y | x}^{-1} \\right| + \\frac{\\lambda}{n}\\left\\| \\beta \\right\\|_{F}^{2} \\right\\} \\] where \\(\\hat{\\Sigma}_{y | x} = (\\mathbb{Y} - \\mathbb{X}\\hat{\\beta} )^{T}(\\mathbb{Y} - \\mathbb{X}\\hat{\\beta})/n\\). In order to solve for \\(\\hat{\\beta}\\), we need to iterate between \\(\\hat{\\beta}\\) and \\(\\hat{\\Sigma}_{y | x}^{-1}\\) until convergence. Note that if \\(r = 1\\) then \\(\\sigma_{y}^{2}\\) can be absorbed into \\(\\tilde{\\lambda}\\) and \\[ \\hat{\\beta} = \\left( \\mathbb{X}^{T}\\mathbb{X} + \\tilde{\\lambda}I_{p} \\right)^{-1}\\mathbb{X}^{T}\\mathbb{Y} = \\mathbb{X}^{T}\\left( \\mathbb{X}\\mathbb{X}^{T} + \\tilde{\\lambda}_{n}I_{n} \\right)^{-1}\\mathbb{Y} \\] 7.2.3 Assume ridge \\(\\Sigma_{xx}^{-1}\\), optimize in-sample prediction error If we use the sample estimate \\(\\hat{\\Sigma}_{xy} = \\mathbb{X}^{T}\\mathbb{Y}/n\\) then \\[ vec\\left( \\hat{\\Sigma}_{xx}^{-1} \\right) = \\left( \\mathbb{X}^{T}\\mathbb{Y}\\mathbb{Y}^{T}\\mathbb{X} \\otimes \\frac{1}{n}\\mathbb{X}^{T}\\mathbb{X} + \\tilde{\\lambda}I_{pp} \\right)^{-1}vec\\left( \\mathbb{X}^{T}\\mathbb{Y}\\mathbb{Y}^{T}\\mathbb{X} \\right) \\] so that \\(\\hat{\\beta} = \\hat{\\Sigma}_{xx}^{-1}\\hat{\\Sigma}_{xy} = \\hat{\\Sigma}_{xx}^{-1}\\mathbb{X}^{T}\\mathbb{Y}/n\\). 7.2.4 Assume ridge \\(\\Sigma_{xx}^{-1}\\), optimize likelihood for \\(\\beta\\) If we use the sample estimate \\(\\hat{\\Sigma}_{xy} = \\mathbb{X}^{T}\\mathbb{Y}/n\\) then \\[ vec\\left( \\hat{\\Sigma}_{xx}^{-1} \\right) = \\left( \\mathbb{X}^{T}\\mathbb{Y}\\hat{\\Sigma}_{y | x}^{-1}\\mathbb{Y}^{T}\\mathbb{X} \\otimes \\frac{1}{n}\\mathbb{X}^{T}\\mathbb{X} + \\tilde{\\lambda}I_{pp} \\right)^{-1}vec\\left( \\mathbb{X}^{T}\\mathbb{Y}\\hat{\\Sigma}_{y | x}^{-1}\\mathbb{Y}^{T}\\mathbb{X} \\right) \\] where \\(\\hat{\\Sigma}_{y | x} = (\\mathbb{Y} - \\mathbb{X}\\hat{\\beta} )^{T}(\\mathbb{Y} - \\mathbb{X}\\hat{\\beta})/n\\) so that in order to solve for \\(\\hat{\\beta} = \\hat{\\Sigma}_{xx}^{-1}\\hat{\\Sigma}_{xy} = \\hat{\\Sigma}_{xx}^{-1}\\mathbb{X}^{T}\\mathbb{Y}/n\\), we need to iterate between \\(\\hat{\\beta}\\) and \\(\\hat{\\Sigma}_{y | x}^{-1}\\) until convergence. 7.2.5 Assume ridge \\(\\Sigma_{xx}^{-1}\\), optimize likelihood for \\(\\Sigma_{xx}^{-1}\\) \\[ \\hat{\\Sigma}_{xx}^{-1} = \\arg\\min_{\\Sigma_{xx}^{-1}}\\left\\{ Tr\\left( S_{xx}\\Sigma_{xx}^{-1} \\right) - log\\left| \\Sigma_{xx}^{-1} \\right| + \\frac{\\lambda}{2}\\left\\| \\Sigma_{xx}^{-1} \\right\\|_{F}^{2} \\right\\} \\] where \\(S_{xx} = \\mathbb{X}^{T}\\mathbb{X}/n\\). Set the gradient equal to zero and decompose \\(\\Sigma_{xx}^{-1} = VDV^{T}\\) where \\(D\\) is a diagonal matrix with diagonal elements equal to the eigen values of \\(\\Sigma_{xx}^{-1}\\) and \\(V\\) is the matrix with corresponding eigen vectors as columns. \\[ S_{xx} = \\Sigma_{xx} - \\lambda\\Sigma_{xx}^{-1} = VD^{-1}V^{T} - \\lambda VDV^{T} = V\\left(D^{-1} - \\lambda D \\right)V{^T} \\] This equivalence implies that \\[ \\phi_{j}\\left( S_{xx} \\right) = \\frac{1}{\\phi_{j}(\\Sigma_{xx}^{-1})} - \\lambda\\phi_{j}\\left( \\Sigma_{xx}^{-1} \\right) \\] where \\(\\phi_{j}(\\cdot)\\) is the \\(j\\)th eigen value. In summary, if we decompose \\(S_{xx} = VQV^{T}\\) then \\[ \\hat{\\Sigma}_{xx}^{-1} = \\frac{1}{2\\lambda}V\\left[ -Q + \\left( Q^{2} + 4\\lambda I_{p} \\right)^{1/2} \\right]V^{T} \\] so that \\(\\hat{\\beta} = \\hat{\\Sigma}_{xx}^{-1}\\Sigma_{xy} = \\hat{\\Sigma}_{xx}^{-1}\\mathbb{X}^{T}\\mathbb{Y}/n\\). 7.2.6 Assume ridge \\(\\Sigma_{xy}\\), optimize in-sample prediction error If we use the sample estimate \\(\\hat{\\Sigma}_{xx} = \\mathbb{X}^{T}\\mathbb{X}/n\\) (assuming \\(\\mathbb{X}^{T}\\mathbb{X}\\) is positive definite) then \\[ \\hat{\\Sigma}_{xy} = \\left[\\left(\\mathbb{X}^{T}\\mathbb{X} \\right)^{-1} + \\tilde{\\lambda}I_{p} \\right]^{-1}\\left(\\mathbb{X}^{T}\\mathbb{X} \\right)^{-1}\\mathbb{X}^{T}\\mathbb{Y}/n = \\left( I_{p} + \\tilde{\\lambda}\\mathbb{X}^{T}\\mathbb{X} \\right)^{-1}\\mathbb{X}^{T}\\mathbb{Y}/n \\] so that 7.2.7 Assume ridge \\(\\Sigma_{xy}\\), optimize likelihood for \\(\\beta\\) If we use the sample estimate \\(\\hat{\\Sigma}_{xx} = \\mathbb{X}^{T}\\mathbb{X}/n\\) (assuming \\(\\mathbb{X}^{T}\\mathbb{X}\\) is positive definite) then where \\(\\hat{\\Sigma}_{y | x} = (\\mathbb{Y} - \\mathbb{X}\\hat{\\beta} )^{T}(\\mathbb{Y} - \\mathbb{X}\\hat{\\beta})/n\\) so that in order to solve for \\(\\hat{\\beta} = \\hat{\\Sigma}_{xx}^{-1}\\hat{\\Sigma}_{xy} = n\\left( \\mathbb{X}^{T}\\mathbb{X} \\right)^{-1}\\hat{\\Sigma}_{xy}\\), we need to iterate between \\(\\hat{\\beta}\\) and \\(\\hat{\\Sigma}_{y | x}^{-1}\\) until convergence. 7.2.8 Assume ridge \\(\\Sigma_{x | y}^{-1}\\), optimize likelihood for \\(\\Sigma_{x | y}^{-1}\\) \\[ \\hat{\\Sigma}_{x | y}^{-1} = \\arg\\min_{\\Sigma_{x | y}^{-1}}\\left\\{ Tr\\left( S_{x | y}\\Sigma_{x | y}^{-1} \\right) - log\\left| \\Sigma_{x | y}^{-1} \\right| + \\frac{\\lambda}{2}\\left\\| \\Sigma_{x | y}^{-1} \\right\\|_{F}^{2} \\right\\} \\] where \\(S_{x | y} = \\left( \\mathbb{X} - \\mathbb{Y}\\hat{\\alpha} \\right)^{T}\\left( \\mathbb{X} - \\mathbb{Y}\\hat{\\alpha} \\right)/n\\) and \\(\\hat{\\alpha} = \\left( \\mathbb{Y}^{T}\\mathbb{Y} \\right)^{-1}\\mathbb{Y}^{T}\\mathbb{X}\\). Following the derivations in section (3.5), if we decompose \\(S_{x | y} = VQV^{T}\\) then \\[ \\hat{\\Sigma}_{x | y}^{-1} = \\frac{1}{2\\lambda}V\\left[ -Q + \\left( Q^{2} + 4\\lambda I_{p} \\right)^{1/2} \\right]V^{T} \\] Using the Woodbury Identity, we find that \\[ \\hat{\\beta} = \\hat{\\Sigma}_{x | y}^{-1}\\Sigma_{xy}\\left( \\Sigma_{yy} + \\Sigma_{xy}^{T}\\hat{\\Sigma}_{x | y}^{-1}\\Sigma_{xy} \\right)^{-1}\\Sigma_{yy} = \\hat{\\Sigma}_{x | y}^{-1}\\hat{\\alpha}^{T}\\left( \\Sigma_{yy}^{-1} + \\hat{\\alpha}\\hat{\\Sigma}_{x | y}^{-1}\\hat{\\alpha} \\right)^{-1} \\] 7.2.9 Assume ridge \\(\\alpha\\), optimize in-sample prediction error for \\(\\alpha\\) \\[ \\hat{\\alpha} = \\arg\\min_{\\alpha}\\left\\{\\frac{1}{2} \\left\\| \\mathbb{X} - \\mathbb{Y}\\alpha \\right\\|_{F}^{2} + \\frac{\\lambda}{2}\\left\\| \\alpha \\right\\|_{F}^{2}\\right\\} \\] Using the Woodbury Identity, we find that \\[ \\hat{\\beta} = \\Sigma_{x | y}^{-1}\\hat{\\alpha}^{T}\\left( \\Sigma_{yy}^{-1} + \\hat{\\alpha}\\Sigma_{x | y}^{-1}\\hat{\\alpha}^{T} \\right)^{-1} \\] 7.2.10 Assume ridge \\(\\alpha\\), optimize likelihood for \\(\\alpha\\) \\[ \\hat{\\alpha} = \\arg\\min_{\\alpha}\\left\\{ \\frac{1}{n} Tr\\left[ \\left(\\mathbb{X} - \\mathbb{Y}\\alpha \\right)^{T}\\left( \\mathbb{X} - \\mathbb{Y}\\alpha \\right)\\Sigma_{x | y}^{-1} \\right] - \\frac{1}{2}log\\left| \\Sigma_{x | y}^{-1} \\right| + \\frac{\\lambda}{n}\\left\\| \\alpha \\right\\|_{F}^{2} \\right\\} \\] where \\(\\hat{\\Sigma}_{x | y} = (\\mathbb{X} - \\mathbb{Y}\\hat{\\alpha} )^{T}(\\mathbb{X} - \\mathbb{Y}\\hat{\\alpha})/n\\). In order to solve for \\(\\hat{\\alpha}\\), we need to iterate between \\(\\hat{\\alpha}\\) and \\(\\hat{\\Sigma}_{x | y}^{-1}\\) until convergence. Again, using the Woodbury Identity, we find that \\[ \\hat{\\beta} = \\hat{\\Sigma}_{x | y}^{-1}\\hat{\\alpha}^{T}\\left( \\Sigma_{yy}^{-1} + \\hat{\\alpha}\\hat{\\Sigma}_{x | y}^{-1}\\hat{\\alpha}^{T} \\right)^{-1} \\] "],
["references.html", "References", " References "]
]
