<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 3 SCPME | Shrinking Characteristics of Precision Matrix Estimators: An Illustration via Regression</title>
  <meta name="description" content="Matt Galloway’s Master’s thesis.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 3 SCPME | Shrinking Characteristics of Precision Matrix Estimators: An Illustration via Regression" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://mattxgalloway.com/oral_manuscript/" />
  
  <meta property="og:description" content="Matt Galloway’s Master’s thesis." />
  <meta name="github-repo" content="MGallow/oral_manuscript" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 SCPME | Shrinking Characteristics of Precision Matrix Estimators: An Illustration via Regression" />
  
  <meta name="twitter:description" content="Matt Galloway’s Master’s thesis." />
  

<meta name="author" content="Matt Galloway">


<meta name="date" content="2019-04-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="precision-matrix-estimation.html">
<link rel="next" href="appendix.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Matt Galloway Oral Manuscript</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="precision-matrix-estimation.html"><a href="precision-matrix-estimation.html"><i class="fa fa-check"></i><b>2</b> Precision Matrix Estimation</a><ul>
<li class="chapter" data-level="2.1" data-path="precision-matrix-estimation.html"><a href="precision-matrix-estimation.html#background"><i class="fa fa-check"></i><b>2.1</b> Background</a></li>
<li class="chapter" data-level="2.2" data-path="precision-matrix-estimation.html"><a href="precision-matrix-estimation.html#admm-algorithm"><i class="fa fa-check"></i><b>2.2</b> ADMM Algorithm</a></li>
<li class="chapter" data-level="2.3" data-path="precision-matrix-estimation.html"><a href="precision-matrix-estimation.html#simulations"><i class="fa fa-check"></i><b>2.3</b> Simulations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="scpme.html"><a href="scpme.html"><i class="fa fa-check"></i><b>3</b> SCPME</a><ul>
<li class="chapter" data-level="3.1" data-path="scpme.html"><a href="scpme.html#augmented-admm-algorithm"><i class="fa fa-check"></i><b>3.1</b> Augmented ADMM Algorithm</a></li>
<li class="chapter" data-level="3.2" data-path="scpme.html"><a href="scpme.html#regression-illustration"><i class="fa fa-check"></i><b>3.2</b> Regression Illustration</a></li>
<li class="chapter" data-level="3.3" data-path="scpme.html"><a href="scpme.html#simulations-1"><i class="fa fa-check"></i><b>3.3</b> Simulations</a></li>
<li class="chapter" data-level="3.4" data-path="scpme.html"><a href="scpme.html#discussion"><i class="fa fa-check"></i><b>3.4</b> Discussion</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>A</b> Appendix</a></li>
<li class="chapter" data-level="B" data-path="admmsigma-r-package.html"><a href="admmsigma-r-package.html"><i class="fa fa-check"></i><b>B</b> <code>ADMMsigma</code> R Package</a><ul>
<li class="chapter" data-level="B.1" data-path="admmsigma-r-package.html"><a href="admmsigma-r-package.html#installation"><i class="fa fa-check"></i><b>B.1</b> Installation</a></li>
<li class="chapter" data-level="B.2" data-path="admmsigma-r-package.html"><a href="admmsigma-r-package.html#tutorial"><i class="fa fa-check"></i><b>B.2</b> Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="scpme-r-package.html"><a href="scpme-r-package.html"><i class="fa fa-check"></i><b>C</b> <code>SCPME</code> R Package</a><ul>
<li class="chapter" data-level="C.1" data-path="scpme-r-package.html"><a href="scpme-r-package.html#installation-1"><i class="fa fa-check"></i><b>C.1</b> Installation</a></li>
<li class="chapter" data-level="C.2" data-path="scpme-r-package.html"><a href="scpme-r-package.html#tutorial-1"><i class="fa fa-check"></i><b>C.2</b> Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Shrinking Characteristics of Precision Matrix Estimators: An Illustration via Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scpme" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> SCPME</h1>
<p>In their 2017 paper, titled <em>Shrinking Characteristics of Precision Matrix Estimators</em>, Aaron Molstad, Ph.D. and Professor Adam Rothman outline a framework to estimate <em>characteristics</em> of precision matrices. This concept, inspired by others like <span class="citation">Cai and Liu (<a href="#ref-cai2011direct">2011</a>)</span>, <span class="citation">Fan, Feng, and Tong (<a href="#ref-fan2012road">2012</a>)</span>, and <span class="citation">Mai, Zou, and Yuan (<a href="#ref-mai2012direct">2012</a>)</span>, exploits the fact that in many predictive models estimation of the precision matrix is only necessary through its product with another feature, such as a mean vector. The example they offer in <span class="citation">Molstad and Rothman (<a href="#ref-molstad2017shrinking">2017</a>)</span> is in the context of Fisher’s linear discriminant analysis model. If a response vector <span class="math inline">\(Y\)</span> is categorical such that <span class="math inline">\(Y\)</span> can take values in <span class="math inline">\(\left\{1, ..., J\right\}\)</span>, then the linear discrimnant analysis model assumes that the design matrix <span class="math inline">\(X\)</span> conditional on the response vector <span class="math inline">\(Y\)</span> is normally distributed:</p>
<p><span class="math display" id="eq:lda">\[\begin{equation}
X | Y = j \sim N_{p}\left(\mu_{j}, \Omega^{-1}\right)
\tag{3.1}
\end{equation}\]</span></p>
<p>for each <span class="math inline">\(j = 1, ..., J\)</span>. One can see from this formulation that clearly an estimation of both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Omega\)</span> are required for this model. However, they note that if prediction is the primary concern, then for a given observation <span class="math inline">\(X_{i}\)</span> only the <em>characteristic</em> <span class="math inline">\(\Omega\left(\mu_{l} - \mu_{m}\right)\)</span> is needed to discern between response categories <span class="math inline">\(l\)</span> and <span class="math inline">\(m\)</span>. In other words, prediction only requires the characteristic <span class="math inline">\(\Omega\left(\mu_{l} - \mu_{m}\right)\)</span> for each <span class="math inline">\(l, m \in \left\{1, ..., J\right\}\)</span> and does <em>not</em> require estimation of the full precision matrix <span class="math inline">\(\Omega\)</span>. <span class="citation">Cai and Liu (<a href="#ref-cai2011direct">2011</a>)</span> were among the first authors to propose estimating this characteristic directly but the interesting facet that distinguishes Molstad and Rothman’s approach is that their framework simultaneously fits the model in <a href="scpme.html#eq:lda">(3.1)</a> and performs variable selection.</p>
<p>The general framework has applications outside of linear discriminant analysis and we will be exploring a regression application in later sections, but first we will outline their approach. The penalty proposed by <span class="citation">Molstad and Rothman (<a href="#ref-molstad2017shrinking">2017</a>)</span> is of the form</p>
<p><span class="math display" id="eq:pen2">\[\begin{equation}
P\left(\Omega\right) = \lambda\left\| A\Omega B - C \right\|_{1}
\tag{3.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(A \in \mathbb{R}^{m \times p}, B \in \mathbb{R}^{p \times q}, \mbox{ and } C \in \mathbb{R}^{m \times q}\)</span> are matrices that are assumed to be known and specified so that solving the full penalized gaussian negative log-likelihood for <span class="math inline">\(\Omega\)</span> results in solving</p>
<p><span class="math display" id="eq:omegaloglik2">\[\begin{equation}
\hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ tr\left(S\Omega\right) - \log\left|\Omega \right| + \lambda\left\| A\Omega B - C \right\|_{1} \right\}
\tag{3.3}
\end{equation}\]</span></p>
<p>A similar estimator was proposed by <span class="citation">Dalal and Rajaratnam (<a href="#ref-dalal2017sparse">2017</a>)</span> when <span class="math inline">\(C = 0\)</span> but here we do not require it. This form of penalty is particularly useful because it is extremely general. Note that by letting matrices <span class="math inline">\(A = I_{p}, B = I_{p}, \mbox{ and } C = 0\)</span>, this penalty reduces to a lasso penalty - but clearly this form allows for much more creative penalties and <span class="math inline">\(A, B, \mbox{ and } C\)</span> can be constructed so that we penalize the sum, absolute value of <em>many</em> characteristics of the precision matrix <span class="math inline">\(\Omega\)</span>. We will explore how to solve for <span class="math inline">\(\hat{\Omega}\)</span> in <a href="scpme.html#eq:omegaloglik2">(3.3)</a> in the next section.</p>
<div id="augmented-admm-algorithm" class="section level2">
<h2><span class="header-section-number">3.1</span> Augmented ADMM Algorithm</h2>
<p>Solving for <span class="math inline">\(\hat{\Omega}\)</span> in <a href="scpme.html#eq:omegaloglik2">(3.3)</a> uses what we are going to call an <em>augmented ADMM algorithm</em>. Molstad and Rothman do not offer a name for this specific algorithm but it leverages the majorize-minimize principle in one of the steps in the algorithm - augmenting the original ADMM algorithm discussed in the previous chapter. Within the context of the proposed penalty, the original ADMM algorithm for precision matrix estimation would consist of iterating over the following three steps:</p>
<p><span class="math display">\[\begin{align}
  \Omega^{k + 1} &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}L_{\rho}(\Omega, Z^{k}, \Lambda^{k}) \\
  Z^{k + 1} &amp;= \arg\min_{Z \in \mathbb{R}^{n \times r}}L_{\rho}(\Omega^{k + 1}, Z, \Lambda^{k}) \\
  \Lambda^{k + 1} &amp;= \Lambda^{k} + \rho\left(A\Omega^{k + 1}B - Z^{k + 1} - C \right)
\end{align}\]</span></p>
<p>where <span class="math inline">\(L\)</span>, the augmented lagrangian, is defined as</p>
<p><span class="math display" id="eq:auglagrange3">\[\begin{equation}
L_{\rho}(\Omega, Z, \Lambda) = f\left(\Omega\right) + g\left(Z\right) + tr\left[\Lambda &#39;\left(A\Omega B - Z - C\right)\right] + \frac{\rho}{2}\left\|A\Omega B - Z - C\right\|_{F}^{2}
\tag{3.4}
\end{equation}\]</span></p>
<p>Similar to the previous chapter, <span class="math inline">\(f\left(\Omega\right) = tr\left(S\Omega\right) - \log\left|\Omega\right|\)</span> and <span class="math inline">\(g\left(Z\right) = \lambda\left\|Z\right\|_{1}\)</span>. In fact, the details of the algorithm thus far are identical to the previous approach except that we are replacing <span class="math inline">\(\Omega - Z\)</span> with <span class="math inline">\(A\Omega B - Z - C\)</span> in the augmented lagrangian and the dual update <span class="math inline">\(\Lambda^{k + 1}\)</span>.</p>
<p>Instead of solving the first step directly, the authors propose an alternative, approximating objective function, which we will denote as <span class="math inline">\(\tilde{L}\)</span>, that is based on the majorize-minimize principle<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. The purpose of this approximating function is the desire to solve the first step of the algorithm in closed-form. The ADMM algorithm with this modification based on majorize-minimize principle is also found in <span class="citation">Lange (<a href="#ref-lange2016mm">2016</a>)</span> but here we define the approximating function as</p>
<p><span class="math display" id="eq:approx">\[\begin{equation}
\begin{split}
  \tilde{L}_{\rho}\left(\Omega, Z^{k}, \Lambda^{k}\right) = f\left(\Omega\right) &amp;+ g\left(Z^{k}\right) + tr\left[(\Lambda^{k})&#39;(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega B - Z^{k} - C \right\|_{F}^{2} \\
  &amp;+ \frac{\rho}{2}vec\left(\Omega - \Omega^{k}\right)&#39; Q\left(\Omega - \Omega^{k}\right)
\tag{3.5}
\end{split}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(Q = \tau I_{p} - \left(A&#39;A \otimes BB&#39;\right)\)</span> and <span class="math inline">\(\tau\)</span> is chosen such that <span class="math inline">\(Q\)</span> is positive definite. Note that if <span class="math inline">\(Q\)</span> is positive definite, then <span class="math inline">\(L_{\rho}\left(\cdot\right) \leq \tilde{L}\left(\cdot\right)\)</span> for all <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(\tilde{L}\)</span> is a majorizing function<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. The <em>augmented ADMM</em> algorithm developed by Molstad and Rothman, which now includes the majorize-minimize principle, consists of the following repeated iterations:</p>
<p><span class="math display" id="eq:augADMM">\[\begin{equation}
\begin{split}
  \Omega^{k + 1} &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{tr\left[\left(S + G^{k}\right)\Omega\right] - \log\left|\Omega\right| + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \right\}\\
  Z^{k + 1} &amp;= \arg\min_{Z \in \mathbb{R}^{n \times r}}\left\{\lambda\left\|Z\right\|_{1} + tr\left[(\Lambda^{k})&#39;(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega B - Z^{k} - C \right\|_{F}^{2} \right\} \\
  \Lambda^{k + 1} &amp;= \Lambda^{k} + \rho\left(A\Omega^{k + 1}B - Z^{k + 1} - C \right)
\tag{3.6}\notag
\end{split}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(G^{k} = \rho A&#39;\left( A\Omega^{k} B - Z^{k} - C + \rho^{-1}\Lambda^{k} \right)B&#39;\)</span>. Each step in this algorithm can now conveniently be solved in closed-form and the full details of each can be found in the appendix <a href="appendix.html#proofOmegak">A.0.6</a>. The following theorem provides the simplified steps in the algorithm.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-3" class="theorem"><strong>Theorem 3.1  (Augmented ADMM Algorithm for Shrinking Characteristics of Precision Matrix Estimators.)  </strong></span>
Define the soft-thresholding function as <span class="math inline">\(\mbox{soft}(a, b) = \mbox{sign}(a)(\left| a \right| - b)_{+}\)</span> and <span class="math inline">\(S\)</span> as the sample covariance matrix. Set <span class="math inline">\(k = 0\)</span> and initialize <span class="math inline">\(Z^{0}, \Lambda^{0}, \Omega^{0}\)</span>, and <span class="math inline">\(\rho\)</span> and repeat steps 1-5 until convergence.</p>
<ol style="list-style-type: decimal">
<li>Compute <span class="math inline">\(G^{k}\)</span>.</li>
</ol>
<p><span class="math display" id="eq:Gk">\[\begin{equation}
G^{k} = \rho A&#39;\left( A\Omega^{k} B - Z^{k} - C + \rho^{-1}\Lambda^{k} \right)B&#39;
\tag{3.7}\notag
\end{equation}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Via spectral decomposition, decompose</li>
</ol>
<p><span class="math display" id="eq:VQV">\[\begin{equation}
S + \left( G^{k} + (G^{k})&#39; \right)/2 - \rho\tau\Omega^{k} = VQV&#39;
\tag{3.8}\notag
\end{equation}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Update <span class="math inline">\(\Omega^{k + 1}\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></li>
</ol>
<p><span class="math display" id="eq:Omegak">\[\begin{equation}
\Omega^{k + 1} = V\left( -Q + (Q^{2} + 4\rho\tau I_{p})^{1/2} \right)V&#39; /(2\rho\tau)
\tag{3.9}
\end{equation}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>Update <span class="math inline">\(Z^{k + 1}\)</span> with element-wise soft-thresholding for the resulting matrix.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></li>
</ol>
<p><span class="math display" id="eq:Zk">\[\begin{equation}
Z^{k + 1} = \mbox{soft}\left( A\Omega^{k + 1}B - C + \rho^{-1}\Lambda^{k}, \rho^{-1}\lambda \right)
\tag{3.10}
\end{equation}\]</span></p>
<ol start="5" style="list-style-type: decimal">
<li>Update <span class="math inline">\(\Lambda^{k + 1}\)</span>.</li>
</ol>
<p><span class="math display" id="eq:Yk">\[\begin{equation}
\Lambda^{k + 1} = \rho\left( A\Omega^{k + 1} B - Z^{k + 1} - C \right)
\tag{3.11}\notag
\end{equation}\]</span></p>
</div>

<div id="stopping-criterion" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Stopping Criterion</h3>
<p>A possibe stopping criterion for this framework is one derived from similar optimality conditions used in the previous chapter in section <a href="precision-matrix-estimation.html#ADMMstop">2.2.2</a>. The primal optimality condition here is that <span class="math inline">\(A\Omega^{k + 1}B - Z^{k + 1} - C = 0\)</span> and the two dual optimality conditions are <span class="math inline">\(0 \in \partial f\left(\Omega^{k + 1}\right) + \left(B(\Lambda^{k + 1})&#39;A + A&#39;\Lambda^{k + 1}B&#39; \right)/2\)</span> and <span class="math inline">\(0 \in \partial g\left(Z^{k + 1}\right) - \Lambda^{k + 1}\)</span>. Similarly, we will define the left-hand side of the primal optimality condition as the primal residual <span class="math inline">\(r^{k + 1} = A\Omega^{k + 1}B - Z^{k + 1} - C\)</span> and the dual residual<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> as</p>
<p><span class="math display" id="eq:stopproof">\[\begin{equation}
s^{k + 1} = \frac{\rho}{2}\left( B(Z^{k + 1} - Z^{k})&#39;A + A&#39;(Z^{k + 1} - Z^{k})B&#39; \right)
\tag{3.12}\notag
\end{equation}\]</span></p>
<p>For proper convergence, we will require that both residuals are approximately equal to zero. Similar to the stopping criterion discussed previously, one possibility is to set <span class="math inline">\(\epsilon^{rel} = \epsilon^{abs} = 10^{-3}\)</span> and stop the algorithm when <span class="math inline">\(\epsilon^{pri} \leq \left\| r^{k + 1} \right\|_{F}\)</span> and <span class="math inline">\(\epsilon^{dual} \leq \left\| s^{k + 1} \right\|_{F}\)</span> where</p>
<p><span class="math display" id="eq:scpmestopping">\[\begin{equation}
\begin{split}
  \epsilon^{pri} &amp;= \sqrt{nr}\epsilon^{abs} + \epsilon^{rel}\max\left\{ \left\| A\Omega^{k + 1}B \right\|_{F}, \left\| Z^{k + 1} \right\|_{F}, \left\| C \right\|_{F} \right\} \\
  \epsilon^{dual} &amp;= p\epsilon^{abs} + \epsilon^{rel}\left\| \left( B(\Lambda^{k + 1})&#39;A + A&#39;\Lambda^{k + 1}B&#39; \right)/2 \right\|_{F}
\tag{3.13}\notag
\end{split}
\end{equation}\]</span></p>
</div>
</div>
<div id="regression-illustration" class="section level2">
<h2><span class="header-section-number">3.2</span> Regression Illustration</h2>
<p>One of the research directions mentioned in <span class="citation">Molstad and Rothman (<a href="#ref-molstad2017shrinking">2017</a>)</span> that was not further explored was the application of the SCPME framework to regression. Utilizing the fact that the population regression coefficient matrix <span class="math inline">\(\beta \equiv \Omega_{x}\Sigma_{xy}\)</span> for predictors, <span class="math inline">\(X\)</span>, and the responses, <span class="math inline">\(Y\)</span>, they point out that their framework could allow for the simultaneous estimation of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\Omega_{x}\)</span>. Like <span class="citation">Witten and Tibshirani (<a href="#ref-witten2009covariance">2009</a>)</span>, this approach would estimate the forward regression coefficient matrix while using shrinkage estimators for the marginal population precision matrix for the predictors. For example, recall that the general optimization problem outlined in the SCPME framework is to estimate <span class="math inline">\(\hat{\Omega}\)</span> such that</p>
<p><span class="math display" id="eq:penloglik3">\[\begin{equation}
  \hat{\Omega} = \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{ tr(S\Omega) - \log\left| \Omega \right| + \lambda\left\| A\Omega B - C \right\|_{1} \right\}
\tag{3.14}\notag
\end{equation}\]</span></p>
<p>If the user specifies that <span class="math inline">\(A = I_{p}, B = \Sigma_{xy}, C = 0, \mbox{ and } \Omega_{x}\)</span> is the precision matrix for the predictors, then the optimization problem of interest is now</p>
<p><span class="math display" id="eq:penloglik4">\[\begin{equation}
  \hat{\Omega}_{x} = \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr(S_{x}\Omega_{x}) - \log\left| \Omega_{x} \right| + \lambda\left\| \Omega_{x} \Sigma_{xy} \right\|_{1} \right\} \\
\tag{3.15}\notag
\end{equation}\]</span></p>
<p>Specifically, this optimization problem has the effect of deriving an estimate of <span class="math inline">\(\Omega_{x}\)</span> while assuming sparsity in the forward regression coefficient <span class="math inline">\(\beta\)</span>. Of course, in practice we do not know the true covariance matrix <span class="math inline">\(\Sigma_{xy}\)</span> but we might consider using the sample estimate <span class="math inline">\(\hat{\Sigma}_{xy} = \sum_{i = 1}^{n}\left(X_{i} - \bar{X}\right)\left(Y_{i} - \bar{Y}\right)&#39;/n\)</span> in place of <span class="math inline">\(\Sigma_{xy}\)</span>. We could then use our estimator, <span class="math inline">\(\hat{\Omega}_{x}\)</span>, to construct the estimated forward regression coefficient matrix <span class="math inline">\(\hat{\beta} = \hat{\Omega}_{x}\hat{\Sigma}_{xy}\)</span>. Estimators such as these are truly novel an can conveniently be estimated by the SCPME framework. Another such estimator that is a product of this new framework is one where we construct <span class="math inline">\(A \mbox{ and } C\)</span> similarly but take <span class="math inline">\(B = \left[ \Sigma_{xy}, I_{p} \right]\)</span> so that the identity matrix is appended to the cross-covariance matrix of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In this case, not only are we assuming that <span class="math inline">\(\beta\)</span> is sparse, but we are also assuming sparsity in <span class="math inline">\(\Omega\)</span>.</p>
<p><span class="math display" id="eq:pen4">\[\begin{equation}
P_{\lambda}\left(\Omega \right) = \lambda\left\| A\Omega B - C \right\|_{1} = \lambda\left\| \Omega\left[\Sigma_{xy}, I_{p}\right] \right\|_{1} = \lambda\left\| \beta \right\|_{1} + \lambda\left\| \Omega \right\|_{1}
\tag{3.16}\notag
\end{equation}\]</span></p>
<p>Like before, we could use <span class="math inline">\(\hat{\Sigma}_{xy}\)</span> as a replacement and take our resulting estimator, <span class="math inline">\(\hat{\Omega}_{x}\)</span>, to construct the estimated forward regression coefficient matrix <span class="math inline">\(\hat{\beta} = \hat{\Omega}_{x}\hat{\Sigma}_{xy}\)</span>. The embedded assumptions here are that not all predictors in <span class="math inline">\(X\)</span> are useful in predicting the response, <span class="math inline">\(Y\)</span>, <em>and</em> that a number of the predictors are conditionally independent of one another. These are assumptions that are quite reasonable in practice and in the next section we offer a short simulation comparing these new estimators to related and competing prediction methods.</p>
</div>
<div id="simulations-1" class="section level2">
<h2><span class="header-section-number">3.3</span> Simulations</h2>
<p>In this simulation, we compare, under various data realizations, the performance of several competing regression prediction methods including the new so-called SCPME regression estimators discussed in the previous section. In total, we consider the following:</p>
<ul>
<li><p><code>OLS</code> = ordinary least squares estimatior. In high dimensional settings (<span class="math inline">\(p &gt;&gt; n\)</span>), the Moore-Penrose estimator is used as a replacement.</p></li>
<li><p><code>ridge</code> = ridge regression estimator.</p></li>
<li><p><code>lasso</code> = lasso regression estimator.</p></li>
<li><p><code>oracleB</code> = oracle estimator for <span class="math inline">\(\beta\)</span>.</p></li>
<li><p><code>oracleO</code> = regression estimator with oracle <span class="math inline">\(\Omega_{x}^{*}\)</span> (let <span class="math inline">\(\beta = \Omega_{x}^{*}\hat{\Sigma}_{xy}\)</span>).</p></li>
<li><p><code>oracleS</code> = regression estimator with oracle <span class="math inline">\(\Sigma_{xy}^{*}\)</span> (let <span class="math inline">\(\beta = \hat{\Omega}_{x}\Sigma_{xy}^{*}\)</span>) and <span class="math inline">\(\Omega_{x}\)</span> is estimated using an elastic-net penalty.</p></li>
<li><p><code>shrinkB</code> = SCPME regression estimator with penalty <span class="math inline">\(\lambda\left\| \Omega_{x}\hat{\Sigma}_{xy} \right\|_{1}\)</span> so that <span class="math inline">\(\beta = \hat{\Omega}_{x}\hat{\Sigma}_{xy}\)</span>.</p></li>
<li><p><code>shrinkBS</code> = SCPME regression estimator with oracle <span class="math inline">\(\Sigma_{xy}^{*}\)</span> so that the penalty is <span class="math inline">\(\lambda\left\| \Omega_{x}\Sigma_{xy}^{*} \right\|_{1}\)</span> and we let <span class="math inline">\(\beta = \hat{\Omega}_{x}\Sigma_{xy}^{*}\)</span>.</p></li>
<li><p><code>shrinkBO</code> = SCPME regression estimator with penalty <span class="math inline">\(\lambda\left\| \Omega_{x}[\hat{\Sigma}_{xy}, I_{p}] \right\|_{1}\)</span> so that <span class="math inline">\(\beta = \hat{\Omega}_{x}\hat{\Sigma}_{xy}\)</span>.</p></li>
<li><p><code>glasso</code> = graphical lasso estimator with penalty <span class="math inline">\(\lambda\left\| \Omega_{x} \right\|_{1}\)</span> so that <span class="math inline">\(\beta = \hat{\Omega}_{x}\hat{\Sigma}_{xy}\)</span>.</p></li>
</ul>
<p>For each estimator, if selection of a tuning parameter is required, the tuning parameter was chosen so that the mean squared prediction error (MSPE) was minimized over 3-fold cross validation.</p>
<p>The data generating procedure for the simulations is the following. The oracle regression coefficient matrix <span class="math inline">\(\beta^{*}\)</span> was constructed so that <span class="math inline">\(\beta^{*} = \mathbb{B} \circ \mathbb{V}\)</span> where <span class="math inline">\(vec\left( \mathbb{B} \right) \sim N_{pr}\left( 0, I_{p} \otimes I_{r}/\sqrt{p} \right)\)</span> and <span class="math inline">\(\mathbb{V} \in \mathbb{R}^{p \times r}\)</span> is a matrix containing <span class="math inline">\(p\)</span> times <span class="math inline">\(r\)</span> random bernoulli draws with 50% probability being equal to one. The covariance matrices <span class="math inline">\(\Sigma_{y | x}\)</span> and <span class="math inline">\(\Sigma_{x}\)</span> were constructed so that <span class="math inline">\(\left( \Sigma_{y | x} \right)_{ij} = 0.7^{\left| i - j \right|}\)</span> and <span class="math inline">\(\left( \Sigma_{x} \right)_{ij} = 0.7^{\left| i - j \right|}\)</span>, respectively. This ensures that their corresponding precision matrices will be tridiagonal and sparse. Then for 100 independent, identically distributed samples, we had <span class="math inline">\(X_{i} \sim N_{p}\left( 0, \Sigma_{x} \right)\)</span> and <span class="math inline">\(E_{i} \sim N_{r}\left( 0, \Sigma_{y | x} \right)\)</span> so that <span class="math inline">\(\mathbb{Y} = \mathbb{X}\beta + \mathbb{E}\)</span> where <span class="math inline">\(\mathbb{X} \in \mathbb{R}^{n \times p}\)</span> and <span class="math inline">\(\mathbb{Y} \in \mathbb{R}^{n \times r}\)</span> are the matrices with stacked rows <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(E_{i}\)</span>, respectively, for <span class="math inline">\(i = 1, ..., n\)</span> and the script notation denotes the fact that the columns have been centered so as to remove the intercept in our prediction models. A sample of an additional 1000 observations was generated similarly for the testing set. Each prediction method was evaluated on both model error and mean squared prediction error.</p>
<p>Figure <a href="scpme.html#fig:scpmesim2">3.1</a> displays the model error for each method by dimension of the design matrix. Here we took the sample size equal to <span class="math inline">\(n = 100\)</span> and the response matrix dimension <span class="math inline">\(r = 10\)</span> with each data generating procedure replicated a total of 20 times. Note <code>OLS</code> and <code>oracleS</code> are not shown due to extremely poor performance. We find that in high dimensional settings, <code>shrinkBO</code> and <code>shrinkB</code> perform increasingly well relative to the others as the predictor dimension increases. <code>shrinkBO</code> performed the best. Interestingly, when <span class="math inline">\(n &gt; p\)</span> <code>shrinkBO</code> is still one of the best-performing estimators - though worse than both <code>lasso</code> and <code>ridge</code> - but the performance of <code>shrinkB</code> decreases drastically (plot not shown).</p>
<p>In the high dimension setting, the oracle estimators <code>oracleO</code> and <code>oracleS</code> performed worse or comparable to the <code>OLS</code> estimator. The poor performance of <code>oracleS</code> is likely due to the fact that the sample estimate of <span class="math inline">\(\Omega_{x}\)</span> is not identifiable when <span class="math inline">\(p &gt; n\)</span>.</p>
<p>The following table shows the average model error for each estimator when <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(p = 150\)</span>, and <span class="math inline">\(r = 10\)</span>.</p>
<!-- <br>\vspace{0.5cm} -->
<!-- ```{r scpmesim1, eval = T, echo = F, fig.cap = "ME by P and R (N = 100, spase = 0.5, sigma = tridiag. High dimension."} -->
<!-- # load data -->
<!-- load("images/sim6.Rdata") -->
<!-- # set P to numeric  -->
<!-- sim6$P %<>% as.character %>% as.numeric -->
<!-- # ME by P and R (N = 100, sparse = 0.5, sigma = "tridiag") -->
<!-- sim6 %>% filter(N == 100, sparse == 0.5, sigma == "tridiag", !model %in% c("OLS", "oracleS"), metric == "ME") %>% group_by(N, P, R, sparse, sigma, model) %>% summarise(mean = mean(Error)) %>% ggplot(aes(P, mean, color = model, linetype = model)) + geom_line() + scale_colour_discrete("") + scale_linetype_manual("", values = rep(c(5,6), 5)) + theme_minimal() + theme(legend.position = "bottom") + facet_wrap(~ R, scales = "free") + xlab("P") + labs(color = "Model") + scale_x_continuous(breaks = c(10, 50, 100, 150)) + ylab("ME") + ggtitle("ME by P and R (N = 100, sparse = 0.5, sigma = tridiag)") -->
<!-- ``` -->
<!-- <br>\vspace{0.5cm} -->

<div class="figure" style="text-align: center"><span id="fig:scpmesim2"></span>
<img src="Manuscript_files/figure-html/scpmesim2-1.png" alt="The oracle precision matrices were tri-diagonal with variable dimension (p) and the data was generated with sample size n = 100 and response vector dimension r = 10. The model errors (ME) for each estimator with variable dimension of the design matrix are plotted. shrinkBO and shrinkB were the two best-performing estimators closely follow by the ridge and lasso regression estimators." width="85%"  />
<p class="caption">
FIGURE 3.1: The oracle precision matrices were tri-diagonal with variable dimension (p) and the data was generated with sample size n = 100 and response vector dimension r = 10. The model errors (ME) for each estimator with variable dimension of the design matrix are plotted. shrinkBO and shrinkB were the two best-performing estimators closely follow by the ridge and lasso regression estimators.
</p>
</div>

<table style="width:40%;">
<caption>Average model error for dimension p = 150.
</caption>
<colgroup>
<col width="15%" />
<col width="12%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">Error</th>
<th align="center">SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">oracleB</td>
<td align="center">0.5056</td>
<td align="center">0.5122</td>
</tr>
<tr class="even">
<td align="center">shrinkBO</td>
<td align="center">2.341</td>
<td align="center">1.014</td>
</tr>
<tr class="odd">
<td align="center">shrinkB</td>
<td align="center">2.419</td>
<td align="center">1.083</td>
</tr>
<tr class="even">
<td align="center">ridge</td>
<td align="center">2.479</td>
<td align="center">1.128</td>
</tr>
<tr class="odd">
<td align="center">lasso</td>
<td align="center">2.627</td>
<td align="center">1.27</td>
</tr>
<tr class="even">
<td align="center">shrinkBS</td>
<td align="center">5.932</td>
<td align="center">4.001</td>
</tr>
<tr class="odd">
<td align="center">glasso</td>
<td align="center">7.57</td>
<td align="center">5.412</td>
</tr>
<tr class="even">
<td align="center">oracleO</td>
<td align="center">13.18</td>
<td align="center">10.11</td>
</tr>
<tr class="odd">
<td align="center">OLS</td>
<td align="center">14.41</td>
<td align="center">11.24</td>
</tr>
<tr class="even">
<td align="center">oracleS</td>
<td align="center">52.07</td>
<td align="center">44.62</td>
</tr>
</tbody>
</table>
<!-- <br>\vspace{0.5cm} -->
<!-- ```{r scpmesim4, eval = T, echo = F, fig.cap = "ME by P and R (N = 1000, spase = 0.5, sigma = tridiag. Low dimension."} -->
<!-- # ME by P and R (N = 1000, sparse = 0.5, sigma = "tridiag") -->
<!-- sim6 %>% filter(N == 1000, sparse == 0.5, model != "OLS") %>% group_by(N, P, R, sparse, sigma, model) %>% summarise(mean = mean(Error)) %>% ggplot(aes(P, mean, color = model, linetype = model)) + geom_line() + scale_colour_discrete("") + scale_linetype_manual("", values = rep(c(5,6), 5)) + theme_minimal() + theme(legend.position = "bottom") + facet_wrap(~ R, scales = "free") + xlab("P") + labs(color = "Model") + scale_x_continuous(breaks = c(10, 50, 100, 150)) + ylab("ME") + ggtitle("ME by P and R (N = 1000, sparse = 0.5, sigma = tridiag)") -->
<!-- ``` -->
<!-- <br>\vspace{0.5cm} -->
<!-- \vspace{0.5cm} -->
<!-- ```{r scpmesim5, eval = T, echo = F, fig.cap = "ME by P (N = 1000, R = 5, spase = 0.5, sigma = tridiag. Low dimension."} -->
<!-- # ME by P (N = 1000, R = 5, sparse = 0.5, sigma = "tridiag") -->
<!-- sim6 %>% filter(N == 1000, R == 5, sparse == 0.5, model != "OLS") %>% group_by(N, P, R, sparse, sigma, model) %>% summarise(mean = mean(Error)) %>% ggplot(aes(P, mean, color = model, linetype = model)) + geom_line() + scale_colour_discrete("") + scale_linetype_manual("", values = rep(c(5,6), 5)) + theme_minimal() + theme(legend.position = "bottom") + xlab("P") + labs(color = "Model") + scale_x_continuous(breaks = c(10, 50, 100, 150)) + ylab("ME") + ggtitle("ME by P (N = 1000, R = 5, sparse = 0.5, sigma = tridiag)") -->
<!-- ``` -->
<!-- \vspace{0.5cm} -->
<!-- \vspace{0.5cm} -->
<!-- ```{r scpmesim6, eval = T, echo = F, fig.cap = "ME by P (N = 1000, R = 5, sparse = 0.5, sigma = tridiag)."} -->
<!-- # ME table (N = 1000, sparse = 0.5, sigma = "tridiag", P = 150, R = 5) -->
<!-- sim6 %>% filter(N == 1000, sparse == 0.5, sigma == "tridiag", P == 150, R == 5) %>% group_by(model) %>% summarise(mean = mean(Error), sd = sd(Error)) %>% arrange(mean) %>% pander(caption = "ME by P (N = 1000, R = 5, sparse = 0.5, sigma = tridiag).") -->
<!-- ``` -->
<!-- \vspace{0.5cm} -->
<div id="regression-simulations-with-covariance-shrinkage" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Regression Simulations with Covariance Shrinkage</h3>
<p>This final simulation was explored due to the fact that each of the SCPME regression estimators require an estimation of the cross-covariance matrix. In the previous simulations, we naively used the maximum likelihood estimate. However, because the data-generating procedure constructs settings that are inherently sparse, we were interested to determine if additional shrinkage of the maximum likelihood estimate for the <em>cross-covariance</em> matrix would also prove beneficial. For instance, in a number of settings, we were finding that the estimator <code>oracleO</code> was performing worse than <code>oracleS</code>. This perhaps suggests that estimating the covariance matrix <span class="math inline">\(\Sigma_{xy}\)</span> well is <em>more</em> important than estimating <span class="math inline">\(\Omega\)</span> well. This simulation explores that theory a bit further.</p>
<p>The data-generating procedure for this simulation is similar to previous one but here we take <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(p = 200\)</span>, <span class="math inline">\(r = 10\)</span>, and, in addition, we multiply the population cross-covariance estimator by a factor of <span class="math inline">\(k\)</span> where <span class="math inline">\(k \in (0.1, 0.2, ..., 0.9, 1)\)</span>. Figure <a href="scpme.html#fig:scpmesim7">3.2</a> plots the MSPE for <code>shrinkB</code> for each tuning parameter, <span class="math inline">\(\lambda\)</span>, and constant, <span class="math inline">\(k\)</span>, pair. Figure <a href="scpme.html#fig:scpmesim9">3.3</a> plots the MSPE similarly for <code>shrinkBO</code>.</p>
<p>Interestingly, we find that in this high dimension setting, it does appear that shrinking the sample covariance matrix by a constant factor helps the overall prediction performance of the SCPME estimators. This is indicated by the fact that the optimal constant, <span class="math inline">\(k\)</span>, hovers between 0.3 and 0.6 for each of the estimators. We also performed a simulation in low dimensions (not shown) but we did not see the same benefit.</p>


<div class="figure" style="text-align: center"><span id="fig:scpmesim7"></span>
<img src="Manuscript_files/figure-html/scpmesim7-1.png" alt="The oracle precision matrices were tri-diagonal with dimension p = 200 and the data was generated with a sample size n = 100 and response vector dimension r = 10. The cross validation MSPE are plotted for each lambda and constant tuning parameter pair. The optimal tuning parameter pair was found to be log10(lam) = -1 and const = 0.6. Note that brighter areas signify smaller losses." width="85%"  />
<p class="caption">
FIGURE 3.2: The oracle precision matrices were tri-diagonal with dimension p = 200 and the data was generated with a sample size n = 100 and response vector dimension r = 10. The cross validation MSPE are plotted for each lambda and constant tuning parameter pair. The optimal tuning parameter pair was found to be log10(lam) = -1 and const = 0.6. Note that brighter areas signify smaller losses.
</p>
</div>

<!-- <br>\vspace{1cm} -->
<!-- ```{r scpmesim8, eval = T, echo = F, fig.cap = "Figure caption here."} -->
<!-- # shrinkB MSPE by lam and const (N = 100, P = 200, R = 10, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 100, P == 200, R == 10, sparse == 0.5, sigma == "compound", model == "shrinkB") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->
<!-- min = d[d$mean == min(d$mean),] -->
<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkB MSPE (N = 100, P = 200, R = 10, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->
<!-- ``` -->
<!-- ##### `shrinkBO` -->
<!-- <br>\vspace{1cm} -->

<div class="figure" style="text-align: center"><span id="fig:scpmesim9"></span>
<img src="Manuscript_files/figure-html/scpmesim9-1.png" alt="The oracle precision matrices were tri-diagonal with dimension p = 200 and the data was generated with a sample size n = 100 and response vector dimension r = 10. The cross validation MSPE are plotted for each lambda and constant tuning parameter pair. The optimal tuning parameter pair was found to be log10(lam) = -0.5 and const = 0.3. Note that brighter areas signify smaller losses." width="85%"  />
<p class="caption">
FIGURE 3.3: The oracle precision matrices were tri-diagonal with dimension p = 200 and the data was generated with a sample size n = 100 and response vector dimension r = 10. The cross validation MSPE are plotted for each lambda and constant tuning parameter pair. The optimal tuning parameter pair was found to be log10(lam) = -0.5 and const = 0.3. Note that brighter areas signify smaller losses.
</p>
</div>

<!-- <br>\vspace{1cm} -->
<!-- ```{r scpmesim10, eval = T, echo = F, fig.cap = "Figure caption here."} -->
<!-- # shrinkBO MSPE by lam and const (N = 100, P = 200, R = 10, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 100, P == 200, R == 10, sparse == 0.5, sigma == "compound", model == "shrinkBO") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->
<!-- min = d[d$mean == min(d$mean),] -->
<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkBO MSPE (N = 100, P = 200, R = 10, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->
<!-- ``` -->
<!-- <br>\vspace{1cm} -->
<!-- #### Low Dimension -->
<!-- ##### `shrinkB` -->
<!-- <br>\vspace{1cm} -->
<!-- ```{r scpmesim11, eval = T, echo = F, fig.cap = "shrinkB MSPE (N = 1000, P = 50, R = 1, sigma = tridiag). Low dimension."} -->
<!-- # shrinkB MSPE by lam and const (N = 1000, P = 50, R = 1, sigma = "tridiag") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "tridiag", model == "shrinkB") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->
<!-- min = d[d$mean == min(d$mean),] -->
<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkB MSPE (N = 1000, P = 50, R = 1, sigma = tridiag)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->
<!-- ``` -->
<!-- <br>\vspace{1cm} -->
<!-- ```{r scpmesim12, eval = T, echo = F, fig.cap = "Figure caption here."} -->
<!-- # shrinkB MSPE by lam and const (N = 1000, P = 50, R = 1, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "compound", model == "shrinkB") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->
<!-- min = d[d$mean == min(d$mean),] -->
<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkB MSPE (N = 1000, P = 50, R = 1, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->
<!-- ``` -->
<!-- #### `shrinkBO` -->
<!-- <br>\vspace{1cm} -->
<!-- ```{r scpmesim13, eval = T, echo = F, fig.cap = "shrinkBO MSPE (N = 1000, P = 50, R = 1, sigma = tridiag). Low dimension."} -->
<!-- # shrinkBO MSPE by lam and const (N = 1000, P = 10, R = 1, sigma = "tridiag") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "tridiag", model == "shrinkBO") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->
<!-- min = d[d$mean == min(d$mean),] -->
<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkBO MSPE (N = 1000, P = 50, R = 1, sigma = tridiag)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->
<!-- ``` -->
<!-- <br>\vspace{1cm} -->
<!-- ```{r scpmesim14, eval = T, echo = F, fig.cap = "Figure caption here."} -->
<!-- # shrinkBO MSPE by lam and const (N = 1000, P = 50, R = 1, sigma = "compound") -->
<!-- d = sigma %>% filter(N == 1000, P == 50, R == 1, sparse == 0.5, sigma == "compound", model == "shrinkBO") %>% group_by(lam, const) %>% summarise(mean = mean(Error)) %>% mutate(augmean = 1/(c(mean))) -->
<!-- min = d[d$mean == min(d$mean),] -->
<!-- d %>% ggplot(aes(const, log10(lam))) + geom_raster(aes(fill = augmean)) + scale_fill_gradientn(colours = colorRampPalette(bluetowhite)(2), guide = "none") + theme_minimal() + labs(title = "shrinkBO MSPE (N = 1000, P = 50, R = 1, sigma = compound)", caption = paste("**Optimal: log10(lam) = ", round(log10(min$lam), 3), ", const = ", round(min$const, 3), sep = "")) -->
<!-- ``` -->
<!-- <br>\vspace{1cm} -->

</div>
</div>
<div id="discussion" class="section level2">
<h2><span class="header-section-number">3.4</span> Discussion</h2>
<p>Apart from the two SCPME estimators discussed in the previous section, the generality of the SCPME framework allows for many previously unconceptualized precision matrix estimators to be explored. One estimator that was concieved of in our work but deserves further attention in future research is of the form</p>
<p><span class="math display" id="eq:penregression">\[\begin{equation}
\hat{\Omega}_{x} = \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr(S_{x}\Omega_{x}) - \log\left| \Omega_{x} \right| + \frac{\lambda}{2}\left\| \mathbb{X}\Omega_{x} \Sigma_{xy} - \mathbb{Y} \right\|_{F}^{2} \right\}
\tag{3.17}\notag
\end{equation}\]</span></p>
<p>For clarity in regards to the SCPME framework, <span class="math inline">\(A = \mathbb{X}, B = \Sigma_{xy}, \mbox{ and } C = \mathbb{Y}\)</span>. As before, <span class="math inline">\(\mathbb{X}\)</span> is the <span class="math inline">\(n \times p\)</span> matrix with rows <span class="math inline">\(X_{i} \in \mathbb{R}^{p}\)</span> for <span class="math inline">\(i = 1,..., n\)</span> and the script notation denotes that the matrix has been column-centered. The matrix <span class="math inline">\(\mathbb{Y}\)</span> is a similar representation for the observed responses <span class="math inline">\(Y_{i} \in \mathbb{R}^{r}\)</span>. Also note that here we are using the Frobenius norm instead of the matrix <span class="math inline">\(l_{1}\)</span>-norm but this replacement requires only a slight modification of the augmented ADMM algorithm for optimization - details of which will be presented later.</p>
<p>This optimization problem estimates a precision matrix that balances minimizing the gaussian negative log-likelihood for <span class="math inline">\(\Omega_{x}\)</span> with minimizing the squared prediction error for the forward regression model. In other words, the objective function aims to penalize estimates, determined by <span class="math inline">\(\lambda\)</span>, that too heavily favor maximizing the marginal likelihood for <span class="math inline">\(X\)</span> over the predictive performance of the conditional model <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. This estimator also reveals an interesting connection to the joint log-likelihood of the two random variables. To see this, let us suppose that we have <span class="math inline">\(n\)</span> independent copies of the random pair <span class="math inline">\((Y_{i}, X_{i})\)</span> and we assume a linear relationship such that</p>
<p><span class="math display">\[\begin{equation}
Y_{i} = \mu_{y} + \beta&#39;\left(X_{i} - \mu_{x}\right) + E_{i}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(E_{i} \sim N_{r}\left( 0, \Omega_{y | x}^{-1} \right)\)</span> and <span class="math inline">\(X_{i} \sim N_{p}\left( \mu_{x}, \Omega_{x}^{-1} \right)\)</span>. This implies that the conditional distribution of <span class="math inline">\(Y_{i}|X_{i}\)</span> is of the form</p>
<p><span class="math display">\[\begin{equation}
Y_{i} | X_{i} \sim N_{r}\left( \mu_{y} + \beta&#39;\left(X_{i} - \mu_{x}\right), \Omega_{y | x}^{-1} \right)
\end{equation}\]</span></p>
<p>We can use this conditional distribution along with the marginal distribution of <span class="math inline">\(X\)</span> to derive the joint log-likelihood of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Recall that <span class="math inline">\(\beta \equiv \Omega_{x}\Sigma_{xy}\)</span> and <span class="math inline">\(S_{x}\)</span> is the marginal sample covariance matrix of <span class="math inline">\(X\)</span>. Without loss of generality, we will assume here that <span class="math inline">\(\mu_{x} = \mu_{y} = 0\)</span>.</p>
<p><span class="math display" id="eq:partialjointloglik">\[\begin{equation}
\begin{split}
  l\left( \Omega_{y | x}, \Omega_{x}, \Sigma_{xy} | Y, X \right) &amp;= constant + \frac{n}{2}\log\left| \Omega_{y | x} \right| - \frac{1}{2}\sum_{i = 1}^{n} tr\left[ \left( Y_{i} - \Sigma_{xy}&#39;\Omega_{x} X_{i} \right)\left( Y_{i} - \Sigma_{xy}&#39;\Omega_{x} X_{i} \right)&#39;\Omega_{y | x} \right] \\
  &amp;+\frac{n}{2}\log\left| \Omega \right| - \frac{1}{2}\sum_{i = 1}^{n}tr\left( X_{i}X_{i}&#39;\Omega_{x} \right) \\
  &amp;= constant + \frac{n}{2}\log\left| \Omega_{y | x} \right| - \frac{1}{2}tr\left[ \left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{xy} \right)&#39;\left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{y | x} \right)\Omega_{y | x} \right] \\
  &amp;+ \frac{n}{2}\log\left| \Omega_{x} \right| - \frac{n}{2}tr\left( S_{x}\Omega_{x} \right)
\tag{3.18}\notag
\end{split}
\end{equation}\]</span></p>
<p>Optimizing this joint log-likelihood with respect to <span class="math inline">\(\Omega_{x}\)</span> reveals strong similiarities to the estimator that was derived from the SCPME framework:</p>
<p><span class="math display" id="eq:omegajointloglik">\[\begin{equation}
\begin{split}
  \hat{\Omega}_{x} &amp;= \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ \frac{1}{2}tr\left[ \left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{xy} \right)&#39;\left( \mathbb{Y} - \mathbb{X}\Omega_{x}\Sigma_{xy} \right)\Omega_{y | x} \right] + \frac{n}{2}tr\left( S_{x}\Omega_{x} \right) - \frac{n}{2}\log\left| \Omega_{x} \right| \right\} \\
  &amp;= \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr\left( S_{x}\Omega_{x} \right) - \log\left| \Omega_{x} \right| + \frac{1}{n}\left\| \left( \mathbb{X}\Omega_{x}\Sigma_{xy} - \mathbb{Y} \right)\Omega_{y | x}^{1/2} \right\|_{F}^{2} \right\}
\tag{3.19}\notag
\end{split}
\end{equation}\]</span></p>
<p>If it is the case that, given <span class="math inline">\(X\)</span>, each of the <span class="math inline">\(r\)</span> responses are pairwise independent with equal variance so that <span class="math inline">\(\Omega_{y | x}^{1/2} = \sigma_{y | x}I_{r}\)</span> and we let <span class="math inline">\(\lambda = 2\sigma_{y | x}^{2}/n\)</span>, then we have that</p>
<p><span class="math display" id="eq:omegajointmle">\[\begin{equation}
\hat{\Omega}_{x} = \arg\min_{\Omega_{x} \in \mathbb{S}_{+}^{p}}\left\{ tr\left( S_{x}\Omega_{x} \right) - \log\left| \Omega_{x} \right| + \frac{\lambda}{2}\left\| \mathbb{X}\Omega_{x}\Sigma_{xy} - \mathbb{Y} \right\|_{F}^{2} \right\}
\tag{3.20}\notag
\end{equation}\]</span></p>
<p>This is exactly the estimator conceived of previously. Of course, throughout this derivation there were several assumptions that were made and in practice we do not know the true values of neither <span class="math inline">\(\Omega_{y | x}\)</span> nor <span class="math inline">\(\Sigma_{xy}\)</span>. However, we can see that this estimator is solving a very similar problem to that of optimizing the joint log-likelihood with respect to <span class="math inline">\(\Omega_{x}\)</span>. We think this estimator and related ones deserve attention in future work.</p>
<p>The algorithm for solving this optimization problem is below. Note that no closed-form solution exists and so we must resort to some iterative algorithm similar to the SCPME augmented ADMM algorithm<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>. The only difference here is that in step four we no longer require elementwise soft-thresholding.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-6" class="theorem"><strong>Theorem 3.2  (Modified Augmented ADMM Algorithm for Shrinking Characteristics of Precision Matrix Estimators with Frobenius Norm)  </strong></span>
Set <span class="math inline">\(k = 0\)</span> and initialize <span class="math inline">\(Z^{0}, \Lambda^{0}, \Omega^{0}\)</span>, and <span class="math inline">\(\rho\)</span> and repeat steps 1-5 until convergence.</p>
<ol style="list-style-type: decimal">
<li><p>Compute <span class="math inline">\(G^{k} = \rho A&#39;\left( A\Omega^{k} B - Z^{k} - C + \rho^{-1}\Lambda^{k} \right)B&#39;\)</span></p></li>
<li><p>Decompose <span class="math inline">\(S + \left( G^{k} + (G^{k})&#39; \right)/2 - \rho\tau\Omega^{k} = VQV&#39;\)</span> (via the spectral decomposition).</p></li>
<li><p>Set <span class="math inline">\(\Omega^{k + 1} = V\left( -Q + (Q^{2} + 4\rho\tau I_{p})^{1/2} \right)V&#39;/(2\rho\tau)\)</span></p></li>
<li><p>Set <span class="math inline">\(Z^{k + 1} = \left[ \rho\left( A\Omega^{k + 1} B - C \right) + \Lambda^{k} \right]/(\lambda + \rho)\)</span></p></li>
<li>Set <span class="math inline">\(\Lambda^{k + 1} = \rho\left( A\Omega^{k + 1} B - Z^{k + 1} - C \right)\)</span></li>
</ol>
</div>


</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-cai2011direct">
<p>Cai, Tony, and Weidong Liu. 2011. “A Direct Estimation Approach to Sparse Linear Discriminant Analysis.” <em>Journal of the American Statistical Association</em> 106 (496). Taylor &amp; Francis: 1566–77.</p>
</div>
<div id="ref-fan2012road">
<p>Fan, Jianqing, Yang Feng, and Xin Tong. 2012. “A Road to Classification in High Dimensional Space: The Regularized Optimal Affine Discriminant.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 74 (4). Wiley Online Library: 745–71.</p>
</div>
<div id="ref-mai2012direct">
<p>Mai, Qing, Hui Zou, and Ming Yuan. 2012. “A Direct Approach to Sparse Discriminant Analysis in Ultra-High Dimensions.” <em>Biometrika</em> 99 (1). Oxford University Press: 29–42.</p>
</div>
<div id="ref-molstad2017shrinking">
<p>Molstad, Aaron J, and Adam J Rothman. 2017. “Shrinking Characteristics of Precision Matrix Estimators.” <em>Biometrika</em>.</p>
</div>
<div id="ref-dalal2017sparse">
<p>Dalal, Onkar, and Bala Rajaratnam. 2017. “Sparse Gaussian Graphical Model Estimation via Alternating Minimization.” <em>Biometrika</em> 104 (2). Oxford University Press: 379–95.</p>
</div>
<div id="ref-lange2016mm">
<p>Lange, Kenneth. 2016. <em>MM Optimization Algorithms</em>. Vol. 147. SIAM.</p>
</div>
<div id="ref-witten2009covariance">
<p>Witten, Daniela M, and Robert Tibshirani. 2009. “Covariance-Regularized Regression and Classification for High Dimensional Problems.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 71 (3). Wiley Online Library: 615–36.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>Further explanation of the majorizing function <a href="scpme.html#eq:approx">(3.5)</a> in section <a href="appendix.html#taylorsexp">A.0.5</a><a href="scpme.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>If <span class="math inline">\(Q\)</span> is positive definite, then <span class="math inline">\(vec\left(\Omega - \Omega^{k} \right)&#39;\rho Q\left(\Omega - \Omega^{k} \right)/2 &gt; 0\)</span> since <span class="math inline">\(\rho &gt; 0\)</span> and <span class="math inline">\(vec\left(\Omega - \Omega^{k}\right)\)</span> is always nonzero whenever <span class="math inline">\(\Omega \neq \Omega^{k}\)</span>.<a href="scpme.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Proof of <a href="scpme.html#eq:Omegak">(3.9)</a> in section <a href="appendix.html#proofOmegak">A.0.6</a><a href="scpme.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>Proof of <a href="scpme.html#eq:Zk">(3.10)</a> in section <a href="appendix.html#proofZk">A.0.7</a><a href="scpme.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Proof of <a href="scpme.html#eq:stopproof">(3.12)</a> in section <a href="appendix.html#proofstopproof">A.0.8</a>.<a href="scpme.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>Proof in section <a href="appendix.html#proofpenregression">A.0.9</a>.<a href="scpme.html#fnref10" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="precision-matrix-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/MGallow/oral_manuscript/edit/master/03-SCPME.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Manuscript.pdf", "Manuscript.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
