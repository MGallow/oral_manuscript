
# `ADMMsigma` R Package

`ADMMsigma` is an R package that estimates a penalized precision matrix, often denoted $\Omega$, via the alternating direction method of multipliers (ADMM) algorithm. Though not the fastest estimation method, the ADMM algorithm is easily adaptable and allows for rapid experimentation by the user -- the primary goal of this package.

The package currently supports a general elastic-net penalty that allows for both ridge and lasso-type penalties as special cases. In particular, the algorithm solves the following optimization problem:

\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{tr(S\Omega) - \log\det\left(\Omega\right) + \lambda\left[\frac{1 - \alpha}{2}\left\|\Omega\right\|_{F}^{2} + \alpha\left\|\Omega\right\|_{1} \right] \right\} \]

where $\lambda > 0$, $0 \leq \alpha \leq 1$ are tuning parameters, $\left\|\cdot \right\|_{F}^{2}$ is the Frobenius norm, and we define $\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|$.

A list of functions contained in the package can be found below:

* `ADMMsigma()` computes the estimated precision matrix (ridge, lasso, and elastic-net type regularization optional)

* `RIDGEsigma()` computes the estimated ridge penalized precision matrix via closed-form solution

* `plot.ADMMsigma()` produces a heat map or line graph for cross validation errors

* `plot.RIDGEsigma()` produces a heat map or line graph for cross validation errors



## Installation

<br>\vspace{0.5cm}
```{r, eval = FALSE}
# The easiest way to install is from CRAN
install.packages("ADMMsigma")

# You can also install the development version from GitHub:
# install.packages("devtools")
devtools::install_github("MGallow/ADMMsigma")
```
<br>\vspace{0.5cm}

This package is hosted on Github at [github.com/MGallow/ADMMsigma](https://github.com/MGallow/ADMMsigma).



## Tutorial

By default, `ADMMsigma` will estimate a penalized precision matrix, $\Omega$, using the elastic-net penalty and choose the optimal $\lambda$ and $\alpha$ tuning parameters. The primary function is simply `ADMMsigma()`. The input value $X$ is an $n \times p$ data matrix so that there are $n$ rows each representing an observation and $p$ columns each representing a unique feauture or variable.

Here, the data matrix is $100 \times 5$ and generated from a multivariate normal distribution with mean zero and tapered oracle covariance matrix $S$. A tapered covariance matrix has an inverse - or precision matrix - that is tri-diagonal. That is, the precision matrix is sparse and the shrinkage penalty used here should prove useful.

<br>\vspace{0.5cm}
```{r, message = FALSE, echo = FALSE}
library(ADMMsigma)

#  generate data from a sparse matrix
# first compute covariance matrix
S = matrix(0.7, nrow = 5, ncol = 5)
for (i in 1:5){
  for (j in 1:5){
    S[i, j] = S[i, j]^abs(i - j)
  }
}

# generate 100 x 5 matrix with rows drawn from iid N_p(0, S)
set.seed(123)
Z = matrix(rnorm(100*5), nrow = 100, ncol = 5)
out = eigen(S, symmetric = TRUE)
S.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors)
X = Z %*% S.sqrt


```

```{r, message = FALSE, echo = TRUE}

# elastic-net type penalty (set tolerance to 1e-8)
ADMMsigma(X, tol.abs = 1e-8, tol.rel = 1e-8)

```
<br>\vspace{0.5cm}

We can see that the optimal $\alpha$ value selected is one. This selection corresponds with a lasso penalty -- a special case of the elastic-net penalty.

We can also explicitly assume sparsity in our estimate - rather than let the package decide - by restricting the class of penalties to the lasso. We do this by simply setting `alpha = 1` in our function:

<br>\vspace{0.5cm}
```{r, message = FALSE, echo = TRUE}

# lasso penalty (default tolerance)
ADMMsigma(X, alpha = 1)

```
<br>\vspace{0.5cm}

`ADMMsigma` also has the capability to provide plots for the cross validation errors. This allows the user to analyze and select the appropriate tuning parameters. In the heatmap plot below, the more bright (white) areas of the heat map correspond to a better tuning parameter selection. In the line graph, each line corresponds to a different $\alpha$ tuning parameter.

<br>\vspace{0.5cm}
```{r ADMMsigmapackage1, message = FALSE, echo = TRUE, fig.cap = "CV heatmap for ADMMsigma tutorial.", out.width = "80%"}

# produce CV heat map for ADMMsigma
ADMM = ADMMsigma(X, tol.abs = 1e-8, tol.rel = 1e-8)
plot(ADMM, type = "heatmap")

```

```{r ADMMsigmapackage2, message = FALSE, echo = TRUE, fig.cap = "CV line graph for ADMMsigma tutorial.", out.width = "80%"}

# produce line graph for CV errors for ADMMsigma
plot(ADMM, type = "line")

```
<br>\vspace{0.5cm}

`ADMMsigma` has a number of more advanced options such as cross validation criteria, regularization path, and parallel processing that are explained in detail on the project website at [mattxgalloway.com/ADMMsigma](http://mattxgalloway.com/ADMMsigma/).
