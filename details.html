<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Oral Manuscript</title>
  <meta name="description" content="This is a working manuscript in preparation for Matt Galloway’s doctoral oral examination.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Oral Manuscript" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a working manuscript in preparation for Matt Galloway’s doctoral oral examination." />
  <meta name="github-repo" content="MGallow/oral_manuscript" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Oral Manuscript" />
  
  <meta name="twitter:description" content="This is a working manuscript in preparation for Matt Galloway’s doctoral oral examination." />
  

<meta name="author" content="Matt Galloway">


<meta name="date" content="2018-06-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="tutorial.html">
<link rel="next" href="simulations.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Matt Galloway Oral Manuscript</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="tutorial.html"><a href="tutorial.html"><i class="fa fa-check"></i><b>3</b> Tutorial</a><ul>
<li class="chapter" data-level="3.1" data-path="tutorial.html"><a href="tutorial.html#usage"><i class="fa fa-check"></i><b>3.1</b> Usage</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="details.html"><a href="details.html"><i class="fa fa-check"></i><b>4</b> Details</a><ul>
<li class="chapter" data-level="4.1" data-path="details.html"><a href="details.html#admm-algorithm"><i class="fa fa-check"></i><b>4.1</b> ADMM Algorithm</a></li>
<li class="chapter" data-level="4.2" data-path="details.html"><a href="details.html#scaled-form-admm"><i class="fa fa-check"></i><b>4.2</b> Scaled-Form ADMM</a></li>
<li class="chapter" data-level="4.3" data-path="details.html"><a href="details.html#algorithm"><i class="fa fa-check"></i><b>4.3</b> Algorithm</a><ul>
<li class="chapter" data-level="4.3.1" data-path="details.html"><a href="details.html#proof-of-1"><i class="fa fa-check"></i><b>4.3.1</b> Proof of (1):</a></li>
<li class="chapter" data-level="4.3.2" data-path="details.html"><a href="details.html#proof-of-2"><i class="fa fa-check"></i><b>4.3.2</b> Proof of (2)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="details.html"><a href="details.html#references"><i class="fa fa-check"></i><b>4.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simulations.html"><a href="simulations.html"><i class="fa fa-check"></i><b>5</b> Simulations</a><ul>
<li class="chapter" data-level="5.0.1" data-path="simulations.html"><a href="simulations.html#compound-symmetric-p-100-n-50"><i class="fa fa-check"></i><b>5.0.1</b> Compound Symmetric: P = 100, N = 50</a></li>
<li class="chapter" data-level="5.0.2" data-path="simulations.html"><a href="simulations.html#compound-symmetric-p-10-n-1000"><i class="fa fa-check"></i><b>5.0.2</b> Compound Symmetric: P = 10, N = 1000</a></li>
<li class="chapter" data-level="5.0.3" data-path="simulations.html"><a href="simulations.html#dense-p-100-n-50"><i class="fa fa-check"></i><b>5.0.3</b> Dense: P = 100, N = 50</a></li>
<li class="chapter" data-level="5.0.4" data-path="simulations.html"><a href="simulations.html#dense-p-10-n-50"><i class="fa fa-check"></i><b>5.0.4</b> Dense: P = 10, N = 50</a></li>
<li class="chapter" data-level="5.0.5" data-path="simulations.html"><a href="simulations.html#tridiagonal-p-100-n-50"><i class="fa fa-check"></i><b>5.0.5</b> Tridiagonal: P = 100, N = 50</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="benchmark.html"><a href="benchmark.html"><i class="fa fa-check"></i><b>6</b> Benchmark</a><ul>
<li class="chapter" data-level="6.0.1" data-path="benchmark.html"><a href="benchmark.html#computer-specs"><i class="fa fa-check"></i><b>6.0.1</b> Computer Specs:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Oral Manuscript</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="details" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Details</h1>
<p>Suppose we want to solve the following optimization problem:</p>

<p>where <span class="math inline">\(x \in \mathbb{R}^{n}, z \in \mathbb{R}^{m}, A \in \mathbb{R}^{p \times n}, B \in \mathbb{R}^{p \times m}\)</span>, and <span class="math inline">\(c \in \mathbb{R}^{p}\)</span>. Following <span class="citation">Boyd et al. (<a href="#ref-boyd2011distributed">2011</a>)</span>, the optimization problem will be introduced in vector form though we will later consider cases where <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> are matrices. We will also assume <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are convex functions. Optimization problems like this arise naturally in several statistics and machine learning applications – particularly regularization methods. For instance, we could take <span class="math inline">\(f\)</span> to be the squared error loss, <span class="math inline">\(g\)</span> to be the <span class="math inline">\(l_{2}\)</span>-norm, <span class="math inline">\(c\)</span> to be equal to zero and <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> to be identity matrices to solve the ridge regression optimization problem.</p>
<p>The <em>augmented lagrangian</em> is constructed as follows:</p>
<p><span class="math display">\[ L_{\rho}(x, z, y) = f(x) + g(z) + y^{T}(Ax + Bz - c) + \frac{\rho}{2}\left\| Ax + Bz - c \right\|_{2}^{2} \]</span></p>
<p>where <span class="math inline">\(y \in \mathbb{R}^{p}\)</span> is the lagrange multiplier and <span class="math inline">\(\rho &gt; 0\)</span> is a scalar. Clearly any minimizer, <span class="math inline">\(p^{*}\)</span>, under the augmented lagrangian is equivalent to that of the lagrangian since any feasible point <span class="math inline">\((x, z)\)</span> satisfies the constraint <span class="math inline">\(\rho\left\| Ax + Bz - c \right\|_{2}^{2}/2 = 0\)</span>.</p>
<p><span class="math display">\[ p^{*} = inf\left\{ f(x) + g(z) | Ax + Bz = c \right\} \]</span></p>
<p>The alternating direction method of multipliers (ADMM) algorithm consists of the following repeated iterations:</p>

<p>A more complete introduction to the algorithm – specifically how it arose out of <em>dual ascent</em> and <em>method of multipliers</em> – can be found in <span class="citation">Boyd et al. (<a href="#ref-boyd2011distributed">2011</a>)</span>.</p>
<p><br></p>
<div id="admm-algorithm" class="section level2">
<h2><span class="header-section-number">4.1</span> ADMM Algorithm</h2>
<p>Now consider the case where <span class="math inline">\(X_{1}, ..., X_{n}\)</span> are iid <span class="math inline">\(N_{p}(\mu, \Sigma)\)</span> random variables and we are tasked with estimating the precision matrix, denoted <span class="math inline">\(\Omega \equiv \Sigma^{-1}\)</span>. The maximum likelihood estimator for <span class="math inline">\(\Omega\)</span> is</p>
<p><span class="math display">\[ \hat{\Omega}_{MLE} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) \right\} \]</span></p>
<p>where <span class="math inline">\(S = \sum_{i = 1}^{n}(X_{i} - \bar{X})(X_{i} - \bar{X})^{T}/n\)</span> and <span class="math inline">\(\bar{X}\)</span> is the sample mean. By setting the gradient equal to zero, we can show that when the solution exists, <span class="math inline">\(\hat{\Omega}_{MLE} = S^{-1}\)</span>.</p>
<p>As in regression settings, we can construct a <em>penalized</em> likelihood estimator by adding a penalty term, <span class="math inline">\(P\left( \Omega \right)\)</span>, to the likelihood:</p>
<p><span class="math display">\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) + P\left( \Omega \right) \right\} \]</span></p>
<p><span class="math inline">\(P\left( \Omega \right)\)</span> is often of the form <span class="math inline">\(P\left(\Omega \right) = \lambda\|\Omega \|_{F}^{2}\)</span> or <span class="math inline">\(P\left(\Omega \right) = \|\Omega\|_{1}\)</span> where <span class="math inline">\(\lambda &gt; 0\)</span>, <span class="math inline">\(\left\|\cdot \right\|_{F}^{2}\)</span> is the Frobenius norm and we define <span class="math inline">\(\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|\)</span>. These penalties are the ridge and lasso, respectively. We will, instead, take <span class="math inline">\(P\left( \Omega \right) = \lambda\left[\frac{1 - \alpha}{2}\left\| \Omega \right|_{F}^{2} + \alpha\left\| \Omega \right\|_{1} \right]\)</span> so that the full penalized likelihood is</p>
<p><span class="math display">\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ Tr\left(S\Omega\right) - \log \det\left(\Omega \right) + \lambda\left[\frac{1 - \alpha}{2}\left\| \Omega \right|_{F}^{2} + \alpha\left\| \Omega \right\|_{1} \right] \right\} \]</span></p>
<p>where <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. This <em>elastic-net</em> penalty was explored by Hui Zou and Trevor Hastie <span class="citation">(Zou and Hastie <a href="#ref-zou2005regularization">2005</a>)</span> and is identical to the penalty used in the popular penalized regression package <code>glmnet</code>. Clearly, when <span class="math inline">\(\alpha = 0\)</span> the elastic-net reduces to a ridge-type penalty and when <span class="math inline">\(\alpha = 1\)</span> it reduces to a lasso-type penalty.</p>
<p>By letting <span class="math inline">\(f\)</span> be equal to the non-penalized likelihood and <span class="math inline">\(g\)</span> equal to <span class="math inline">\(P\left( \Omega \right)\)</span>, our goal is to minimize the full augmented lagrangian where the constraint is that <span class="math inline">\(\Omega - Z\)</span> is equal to zero:</p>
<p><span class="math display">\[ L_{\rho}(\Omega, Z, \Lambda) = f\left(\Omega\right) + g\left(Z\right) + Tr\left[\Lambda\left(\Omega - Z\right)\right] + \frac{\rho}{2}\left\|\Omega - Z\right\|_{F}^{2} \]</span></p>
<p>The ADMM algorithm for estimating the penalized precision matrix in this problem is</p>

<p><br></p>
</div>
<div id="scaled-form-admm" class="section level2">
<h2><span class="header-section-number">4.2</span> Scaled-Form ADMM</h2>
<p>An alternate form of the ADMM algorithm can constructed by scaling the dual variable (<span class="math inline">\(\Lambda^{k}\)</span>). Let us define <span class="math inline">\(R^{k} = \Omega - Z^{k}\)</span> and <span class="math inline">\(U^{k} = \Lambda^{k}/\rho\)</span>.</p>

<p>Therefore, the condensed-form ADMM algorithm can now be written as</p>

<p>And more generally (in vector form),</p>

<p>Note that there are limitations to using this method. Because the dual variable is scaled by <span class="math inline">\(\rho\)</span> (the step size), this form limits one to using a constant step size without making further adjustments to <span class="math inline">\(U^{k}\)</span>. It has been shown in the literature that a dynamic step size can significantly reduce the number of iterations required for convergence.</p>
<p><br></p>
</div>
<div id="algorithm" class="section level2">
<h2><span class="header-section-number">4.3</span> Algorithm</h2>

<p><br></p>
<p>Initialize <span class="math inline">\(Z^{0}, \Lambda^{0}\)</span>, and <span class="math inline">\(\rho\)</span>. Iterate the following three steps until convergence:</p>
<ol style="list-style-type: decimal">
<li>Decompose <span class="math inline">\(S + \Lambda^{k} - \rho Z^{k} = VQV^{T}\)</span> via spectral decomposition.</li>
</ol>
<p><span class="math display">\[ \Omega^{k + 1} = \frac{1}{2\rho}V\left[ -Q + \left( Q^{2} + 4\rho I_{p} \right)^{1/2} \right]V^{T} \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Elementwise soft-thresholding for all <span class="math inline">\(i = 1,..., p\)</span> and <span class="math inline">\(j = 1,..., p\)</span>.</li>
</ol>

<ol start="3" style="list-style-type: decimal">
<li>Update <span class="math inline">\(\Lambda^{k + 1}\)</span>.</li>
</ol>
<p><span class="math display">\[ \Lambda^{k + 1} = \Lambda^{k} + \rho\left( \Omega^{k + 1} - Z^{k + 1} \right) \]</span></p>
<p><br></p>
<div id="proof-of-1" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Proof of (1):</h3>
<p><span class="math display">\[ \Omega^{k + 1} = \arg\min_{\Omega}\left\{ Tr\left(S\Omega\right) - \log\det\left(\Omega\right) + Tr\left[\Lambda^{k}\left(\Omega - Z^{k}\right)\right] + \frac{\rho}{2}\left\| \Omega - Z^{k} \right\|_{F}^{2} \right\} \]</span></p>

<p>Set the gradient equal to zero and decompose <span class="math inline">\(\Omega = VDV^{T}\)</span> where <span class="math inline">\(D\)</span> is a diagonal matrix with diagonal elements equal to the eigen values of <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(V\)</span> is the matrix with corresponding eigen vectors as columns.</p>
<p><span class="math display">\[ S + \Lambda^{k} - \rho Z^{k} = \Omega^{-1} - \rho \Omega = VD^{-1}V^{T} - \rho VDV^{T} =  V\left(D^{-1} - \rho D\right)V^{T} \]</span></p>
<p>This equivalence implies that</p>
<p><span class="math display">\[ \phi_{j}\left( S + \Lambda^{k} - \rho Z^{k} \right) = \frac{1}{\phi_{j}(\Omega^{k + 1})} - \rho\phi_{j}(\Omega^{k + 1}) \]</span></p>
<p>where <span class="math inline">\(\phi_{j}(\cdot)\)</span> is the <span class="math inline">\(j\)</span>th eigen value.</p>

<p>In summary, if we decompose <span class="math inline">\(S + \Lambda^{k} - \rho Z^{k} = VQV^{T}\)</span> then</p>
<p><span class="math display">\[ \Omega^{k + 1} = \frac{1}{2\rho}V\left[ -Q + (Q^{2} + 4\rho I_{p})^{1/2}\right] V^{T} \]</span></p>
<p><br></p>
</div>
<div id="proof-of-2" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Proof of (2)</h3>
<p><span class="math display">\[ Z^{k + 1} = \arg\min_{Z}\left\{ \lambda\left[ \frac{1 - \alpha}{2}\left\| Z \right\|_{F}^{2} + \alpha\left\| Z \right\|_{1} \right] + Tr\left[\Lambda^{k}\left(\Omega^{k + 1} - Z\right)\right] + \frac{\rho}{2}\left\| \Omega^{k + 1} - Z \right\|_{F}^{2} \right\} \]</span></p>

<p>where <span class="math inline">\(Sign(Z)\)</span> is the elementwise Sign operator. By setting the gradient/sub-differential equal to zero, we arrive at the following equivalence:</p>
<p><span class="math display">\[ Z_{ij} = \frac{1}{\lambda(1 - \alpha) + \rho}\left( \rho \Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} - Sign(Z_{ij})\lambda\alpha \right) \]</span></p>
<p>for all <span class="math inline">\(i = 1,..., p\)</span> and <span class="math inline">\(j = 1,..., p\)</span>. We observe two scenarios:</p>
<ul>
<li>If <span class="math inline">\(Z_{ij} &gt; 0\)</span> then</li>
</ul>
<p><span class="math display">\[ \rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} &gt; \lambda\alpha \]</span></p>
<ul>
<li>If <span class="math inline">\(Z_{ij} &lt; 0\)</span> then</li>
</ul>
<p><span class="math display">\[ \rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k} &lt; -\lambda\alpha \]</span></p>
<p>This implies that <span class="math inline">\(Sign(Z_{ij}) = Sign(\rho\Omega_{ij}^{k + 1} + \Lambda_{ij}^{k})\)</span>. Putting all the pieces together, we arrive at</p>

<p>where <span class="math inline">\(Soft\)</span> is the soft-thresholding function.</p>
<p><br><br></p>
<!-- \begin{thebibliography}{9} -->
<!-- \bibitem{1} -->
<!--   Boyd, Stephen, et al. "Distributed optimization and statistical learning via the alternating direction method of multipliers." Foundations and Trends® in Machine Learning 3.1 (2011): 1-122. -->
<!-- \bibitem{2} -->
<!--   Polson, Nicholas G., James G. Scott, and Brandon T. Willard. "Proximal algorithms in statistics and machine learning." Statistical Science 30.4 (2015): 559-581. -->
<!-- \bibitem{3} -->
<!--   Marjanovic, Goran, and Victor Solo. "On $ l_q $ optimization and matrix completion." IEEE Transactions on signal processing 60.11 (2012): 5714-5724. -->
<!-- \bibitem{4} -->
<!--   Zou, Hui, and Trevor Hastie. "Regularization and variable selection via the elastic net." Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67.2 (2005): 301-320. -->
<!-- \end{thebibliography} -->
</div>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">4.4</span> References</h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-boyd2011distributed">
<p>Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. “Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.” <em>Foundations and Trends in Machine Learning</em> 3 (1). Now Publishers, Inc.: 1–122.</p>
</div>
<div id="ref-zou2005regularization">
<p>Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67 (2). Wiley Online Library: 301–20.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tutorial.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simulations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/MGallow/oral_manuscript/edit/master/03-Details.Rmd",
"text": "Edit"
},
"download": ["Manuscript.pdf", "Manuscript.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
